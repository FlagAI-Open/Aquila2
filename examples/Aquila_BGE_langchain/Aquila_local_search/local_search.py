import sys
import json
import os
import shutil
import time
from typing import List
import gradio as gr
import nltk
import requests




from langchain.chains.base import Chain
from utils.model_config import *
from aqulia_server_chain import AquliaModel

sys.path.insert(0, "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE")

import argparse
from tool import SearchTool

LLM_MODEL = "aquilachat2-34b"

BGE_DATA_PATH = "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE/data/ai_filter.json"
BGE_ABSTRACT_EMB_PATH = "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE/data/abstract.npy"
BGE_ABSTRACT_INDEX_PATH = "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE/abstract.index"
BGE_ABSTRACT_BM25_INDEX_PATH = "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE/abstract_bm25_index"
BGE_META_EMB_PATH = "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE/data/meta.npy"
BGE_META_INDEX_PATH = "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE/meta.index"
BGE_META_BM25_INDEX_PATH = "/mnt/share/baaisolution/zhaolulu/Aquila_BGE_langchain/BGE/meta_bm25_index"
BGE_BATCH_SIZE = 128
BGE_SEARCH_NUM = 3

llm_model_dict = {
    "aquilachat2-34b": {
        "name": "aquilachat2-34b",
        "pretrained_model_name": "aquilachat2-34b",
        "local_model_path": "/mnt/share/baaisolution/zhaolulu/Aquila2/checkpoints",
        "provides": "Aquila"
    }
}

def generate_prompt(related_docs, query, prompt_template: str = PROMPT_TEMPLATE, ) -> str:
    context = []
    for idx, doc in related_docs.items():
        content = "The abstract is: " + doc["content"]["abstract"].replace("\n", " ") + " " + "The title is: " + doc["content"]["title"].replace(
            "\n", " ") + " " + "The author is: " + ",".join(doc["content"]["authors"]) + "."
        content = "source "+str(idx) + ": " + content

        context.append(content)
    context = "\n\n".join(context)

    prompt = prompt_template.replace("{question}", query).replace("{context}", context)
    return prompt


def torch_gc():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    elif torch.backends.mps.is_available():
        try:
            from torch.mps import empty_cache
            empty_cache()
        except Exception as e:
            print(e)
            print("å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ macOS å»ºè®®å°† pytorch ç‰ˆæœ¬å‡çº§è‡³ 2.0.0 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œä»¥æ”¯æŒåŠæ—¶æ¸…ç† torch äº§ç”Ÿçš„å†…å­˜å ç”¨ã€‚")

class MyLocalDocQA():
    def __init__(self):
        self.llm_model_chain: Chain = None

    def init_cfg(self,llm_model: Chain = None):
        self.llm_model_chain = llm_model

    def get_search_result_from_BGE(self, query):
        search_tool = SearchTool(BGE_DATA_PATH,
                             BGE_ABSTRACT_EMB_PATH,
                             BGE_ABSTRACT_INDEX_PATH,
                             BGE_ABSTRACT_BM25_INDEX_PATH,
                             BGE_META_EMB_PATH,
                             BGE_META_INDEX_PATH,
                             BGE_META_BM25_INDEX_PATH,
                             128)

        input_text = query
        retrieval_type = "merge"
        query_type = "by query"
        target_type = "conditional"
        num = BGE_SEARCH_NUM
        rerank = "enable"
        rerank_num = 25
        response = search_tool.search(input_text, retrieval_type, query_type, target_type, num, rerank, rerank_num)
        return response

    def get_knowledge_based_answer_Aquila2_34b(self, query, chat_history):
        related_docs_with_score = self.get_search_result_from_BGE(query)
        torch_gc()
        if len(related_docs_with_score) > 0:
            prompt = generate_prompt(related_docs_with_score, query)
        else:
            prompt = query
        tokenizer = self.llm_model_chain.tokenizer
        answer = self.llm_model_chain.model.predict(prompt, tokenizer=tokenizer, model_name="aquilachat2-34b", max_gen_len=512)
        msg = ""
        for ans in answer.split():
            msg += ans
            msg += " "
            res = {"query": query,
                   "result": msg,
                   "source_documents": related_docs_with_score}
            chat_history[-1][0] = query
            if len(msg) % 5 == 0:
                time.sleep(0.25)
                yield res, chat_history
        yield res, chat_history
    

llm_model_dict_list = list(llm_model_dict.keys())

local_doc_qa = MyLocalDocQA()

flag_csv_logger = gr.CSVLogger()


def get_answer(query, history, mode):
    
    if mode == "æ–‡çŒ®åº“é—®ç­”":
        logger.info("Use Aquila model to generate answer")
        for resp, res_history in local_doc_qa.get_knowledge_based_answer_Aquila2_34b(
                    query=query, chat_history=history + [[query, ""]]):
                source = f"{resp['result']}\n\n"
                #logger.info("source_documents is [{}]".format(resp["source_documents"]))
                source += "".join(
                    [f"""<details> <summary>å‡ºå¤„ [{int(i) + 1}]:{doc["content"].get("title","")} </summary>\n"""
                     f"""{json.dumps(doc["content"],ensure_ascii=False)}\n"""
                     f"""</details>"""
                     for i, doc in resp["source_documents"].items()])

                res_history[-1][-1] = source
                yield res_history, ""

    logger.info(f"flagging: username={FLAG_USER_NAME},query={query},mode={mode},history={history}")
    flag_csv_logger.flag([query, history, mode], username=FLAG_USER_NAME)


def clear_history(request: gr.Request):
    return "", None


def init_model():
    if "aquilachat" in LLM_MODEL.lower():
        llm_model_ins = AquliaModel(llm_model_dict[LLM_MODEL])

    else:
        logger.info("åŠ è½½æ¨¡å‹ä¸æ˜¯Aquilaç³»åˆ—ï¼Œè¯·æ‰‹åŠ¨æ·»åŠ éœ€è¦åŠ è½½çš„æ¨¡å‹")

    try:
        local_doc_qa.init_cfg(llm_model=llm_model_ins)
        reply = """æ¨¡å‹å’Œæ•°æ®å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯æŸ¥è¯¢ï¼Œæˆ–ä»å³ä¾§é€‰æ‹©æ¨¡å¼åå¼€å§‹å¯¹è¯"""
        logger.info(reply)
      
        return reply
    except Exception as e:
        logger.error(e)
        reply = """æ¨¡å‹æœªæˆåŠŸåŠ è½½"""
        if str(e) == "Unknown platform: darwin":
            logger.info("è¯¥æŠ¥é”™å¯èƒ½å› ä¸ºæ‚¨ä½¿ç”¨çš„æ˜¯ macOS æ“ä½œç³»ç»Ÿï¼Œéœ€å…ˆä¸‹è½½æ¨¡å‹è‡³æœ¬åœ°åæ‰§è¡Œ Web UIï¼Œå…·ä½“æ–¹æ³•è¯·å‚è€ƒé¡¹ç›® README ä¸­æœ¬åœ°éƒ¨ç½²æ–¹æ³•åŠå¸¸è§é—®é¢˜ï¼š"
                        " https://github.com/imClumsyPanda/langchain-ChatGLM")
        else:
            logger.info(reply)
        return reply




def change_mode(mode, history):
    if mode == "æ–‡çŒ®åº“é—®ç­”":
        return gr.update(visible=True), gr.update(visible=False), history
    else:
        return gr.update(visible=False), gr.update(visible=False), history


block_css = """.importantButton {
    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
    border: none !important;
}
.importantButton:hover {
    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
    border: none !important;
}"""

webui_title = """
# ğŸ‰ BAAI-LLM-SearchSystem !!! ğŸ‰
ğŸ‘ [å®ç°ä»£ç ç‚¹å‡»é“¾æ¥è·³è½¬](https://gitee.com/BaaiAC/llm_app_model_service_hub.git)
"""
init_message = f""" æ¬¢è¿ä½¿ç”¨ BAAI-LLM-SearchSystemæŸ¥è¯¢ç³»ç»Ÿ!!

æ”¯æŒæœ¬åœ°æ–‡çŒ®åº“é—®ç­”æŸ¥è¯¢ã€‚

"""

# åˆå§‹åŒ–æ¶ˆæ¯
model_status = init_model()

default_theme_args = dict(
    font=["Source Sans Pro", 'ui-sans-serif', 'system-ui', 'sans-serif'],
    font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],
)


with gr.Blocks(css=block_css, theme=gr.themes.Default(**default_theme_args)) as demo:
    file_status, model_status = gr.State(""), gr.State(model_status)
    gr.Markdown(webui_title)
    with gr.Tab("æ–‡çŒ®åº“æ£€ç´¢"):
        with gr.Row():
            with gr.Column(scale=15):
                chatbot = gr.Chatbot([[None, init_message], [None, model_status.value]],
                                     elem_id="chat-box",
                                     show_label=False).style(height=600)
                query = gr.Textbox(show_label=False,
                                   placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
                # è¿™é‡Œè·å–query
                clear_btn = gr.Button(value="æ–°å¯¹è¯", interactive=True, elem_id="clear_btn")

            with gr.Column(scale=5):
                mode = gr.Radio(["æ–‡çŒ®åº“é—®ç­”"],
                                label="è¯·é€‰æ‹©ä½¿ç”¨æ¨¡å¼",
                                value="æ–‡çŒ®åº“é—®ç­”", )

                examples = [
                    'find papers named "LSTM"',
                    'find papers on machine translation',
                    'can you give me one paper about CNN, show the title and authors?',
                    'find papers about "self attention", use all the abstract to generate a summary',
                    'give me some papers about Transformer and give one summary about these paper',
                    'èƒ½å¸®æˆ‘æ‰¾ä¸€ç¯‡å…‰åˆä½œç”¨ç›¸å…³çš„æ–‡ç« å—?'
                ]

                gr.Examples(examples, query, examples_per_page=20)

                flag_csv_logger.setup([query, chatbot, mode], "flagged")
                mode.change(fn=change_mode,
                            inputs=[mode, chatbot],
                            outputs=[chatbot])
                query.submit(get_answer,
                             [query, chatbot, mode],
                             [chatbot, query])
                clear_btn.click(clear_history, None, [chatbot, query])


(demo.queue(concurrency_count=3).launch(server_name='0.0.0.0',
                                        server_port=9172,
                                        show_api=False,
                                        share=False,
                                        inbrowser=False))
