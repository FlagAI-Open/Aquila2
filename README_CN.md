<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
    <img src="./assets/logo.png" width="500"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/BAAI">Hugging Face</a>&nbsp&nbsp | &nbsp <a href="https://model.baai.ac.cn/models">ModelHub</a>&nbsp&nbsp | &nbsp&nbsp <a href="assets/wechat-qrcode.png">å¾®ä¿¡</a>
</p>
<br><br>

æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„ **Aquila2** ç³»åˆ—ï¼Œç°åœ¨åŒ…æ‹¬åŸºç¡€è¯­è¨€æ¨¡å‹ **Aquila2-7B** å’Œ **Aquila2-34B** ï¼Œä»¥åŠå¯¹è¯æ¨¡å‹ **Aquila2-7B-Chat** å’Œ **Aquila2-34B-Chat**ã€‚

| æ¨¡å‹åç§°         | Modelhub  | Huggingface | 
|----------------------|:----:|:-----------: |
| Aquila2-7B | https://model.baai.ac.cn/model-detail/100118 |    -     | 
| AquilaChat2-7B | https://model.baai.ac.cn/model-detail/100117 |   -      | 
| Aquila2-34B | https://model.baai.ac.cn/model-detail/100119  |    -    | 
| AquilaChat2-34B | https://model.baai.ac.cn/model-detail/100116 |   -      |

åœ¨è¿™ä¸ªä»“åº“ä¸­ï¼Œæ‚¨å¯ä»¥ï¼š

* å¿«é€Ÿå¼€å§‹ä½¿ç”¨ Aquila2ï¼Œè¿›è¡Œç®€å•çš„æ¨ç†ã€‚
* æœ‰å…³é‡åŒ–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ–¹æ³•ã€å†…å­˜ã€æ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†æ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº† BF16 æ¨¡å‹çš„ç»Ÿè®¡æ•°æ®ã€‚
* å¾®è°ƒæ•™ç¨‹ï¼ŒåŒ…æ‹¬å…¨å‚æ•°è°ƒä¼˜ã€LoRA å’Œ Q-LoRAã€‚
* é•¿æ–‡æœ¬ç†è§£è¯„ä¼°çš„ç»Ÿè®¡æ•°æ®
* è®¸å¯åè®®
* ...

æ¬¢è¿å¯¹æˆ‘ä»¬æå‡ºä»»ä½•é—®é¢˜ï¼ˆå»ºè®®ç”¨è‹±è¯­ï¼Œè¿™æ ·æ›´å¤šäººä¼šæ˜ç™½ä½ çš„é—®é¢˜å“¦ï¼‰ï¼å¦‚æœæœ‰å…´è¶£å¸®æˆ‘ä»¬æ”¹è¿› **Aquila2**ï¼Œå¯ä»¥æäº¤ä½ çš„Pull Requestsï¼Œ æˆ‘ä»¬ä¼šåŠæ—¶å¤„ç†ã€‚

å¦‚æœä½ æƒ³ä¸æˆ‘ä»¬è¿›è¡Œè®¨è®ºå’Œäº¤æµï¼Œè¯·å°½å¿«åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å§(è¯·å‚è§æ–‡æ¡£é¡¶éƒ¨ä»¥è·å–å…¥å£ä¿¡æ¯)ï¼



<br>

## æ›´æ–°

* 2023.10.10 ğŸ”¥ æˆ‘ä»¬åœ¨ ModelHub å’Œ Hugging Face ä¸Šå‘å¸ƒäº† **Aquila2-34B** å’Œ **Aquila2-34B-Chat**ã€‚

## è¯„æµ‹è¡¨ç°

Aquila2-34Bå’ŒAquila2-7Bï¼ˆæœ€æ–°ç‰ˆæœ¬ä½¿ç”¨äº†æ›´å¤šæ•°æ®å’Œæ›´é•¿çš„ä¸Šä¸‹æ–‡è¿›è¡Œäº†è®­ç»ƒï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä»2048æ‰©å±•åˆ°äº†8192ï¼‰ç›¸æ¯”åŒè§„æ¨¡çš„åŸºçº¿æ¨¡å‹åœ¨å„é¡¹è¯„æµ‹æ•°æ®é›†ä¸Šå‡è¡¨ç°æ›´ä¼˜ï¼Œè¯„æµ‹æ•°æ®é›†åŒ…æ‹¬MMLUã€C-Evalã€GSM8Kã€MATHã€HumanEvalç­‰ï¼Œè€ƒå¯Ÿäº†æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€æ•°å­¦é—®é¢˜æ±‚è§£èƒ½åŠ›ã€ä»£ç èƒ½åŠ›ç­‰å„æ–¹é¢èƒ½åŠ›ã€‚

<p align="left">
    <img src="assets/radar_34b.jpg" width="600"/>
<p>
<br>

| Model              | C-Eval   | MMLU     | CMMLU    | GSM8K    | GaoKao    | MATH     | HumanEval | WMT22 (en-zh) | WinoGranded |
|:-------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:---------:|:-------------:|:-----------:|
|                    |          |          |          |          |           | 4-shot   |           |               |             |
| InternLM-7B        | 53.4     | 51.0     |          | 31.2     |           | 7.1?6.3  | 10.4      | 53.3          | 68.2        |
| InternLM-20B       | 58.8     | 62.1     |          | 52.6     |           | 7.9      | 25.6      | 56.9          | 69.4        |
| ChatGLM2-6B        | 51.7     | 47.9     | 48.8     | 32.4     | 49.4      | 6.5      | 9.2       | 45.7          |             |
| ChatGLM2-12B       | 61.6     | 56.2     |          | 40.9     |           |          |           |               |             |
| Baichuan2-7B       | 54.0     | 54.2     | 57.1     | 24.5     | 47.5?38.0 | 5.6      | 18.3      | 55.9          |             |
| Baichuan2-13B      | 58.1     | 59.2     | 62.0     | 52.8     | 54.3      | 10.1     | 17.1      | 60.5          | 67.3?70.3   |
| Qwen-7b            | 59.6     | 56.7     | 58.8     | 51.6     |           | 11.1     | 24.4      | 58.1          | 66.1        |
| Qwen-14b           | 72.1     | 66.3     | 71.0     | 61.3     |           | 24.8     | 32.3      | 55.4          |             |
| LLaMA2-7B          | 28.9     | 45.7     | 31.4     | 16.2     | 26.0      | 3.2      | 12.8      | 36.4          |             |
| LLaMA2-13B         |          |          |          |          |           |          |           |               |             |
| LLaMA2-34B         |          |          |          |          |           |          |           |               |             |
| LLaMA2-70B         |          |          |          |          |           | 12.0     |           |               | 69.8        |
| **Aquila2-7B**     | 53.0     | 46.2     | 44.5     |          |           |          |           |               |             |
| **Aquila2-33B**    |          |          |          |          |           |          |           |               |             |

<br><br>

### é•¿æ–‡æœ¬ä»»åŠ¡è¡¨ç°
| Model                     | Method             | SingleQA | MultiQA | Summarization | Code Completion | Few Shot | Synthetics | Other |
|:--------------------------|:------------------:|:--------:|:-------:|:-------------:|:---------------:|:--------:|:----------:|:-----:|
| GPT-3.5-Turbo-16K         | Unknown            | 47.6     | 36.2    | 23.0          | 54.5            | 77.5     | 27.5       | 35.3  |
| LongChat-7B-v1.5-32K      | PI+SFT             | 27.5     | 20.3    | 22.5          | 57.0            | 62.9     | 18.8       | 23.7  |
| ChatGLM2-6B-32K           | Continual Pretrain | 44.1     | 34.7    | 20.8          | 52.7            | 68.7     | 23.6       | 31.6  |
| Baichuan2-13B-Chat-16K    | None               | 14.5     | 12.6    | 14.0          |                 | 22.2     | 11.9       | 14.7  |
| Qwen-14B-Chat-dynamic-ntk | Dynamic NTK        | 24.4     | 20.2    | 22.3          | 37.3            | 47.2     | 18.6       | 24.0      |
| Internlm-20B-Chat         | None               | 19.2     | 17.5    | 16.7          | 26.0            | 41.0     | 16.5       | 19.7  |
| Aquila2-7B-16K            | PI+SFT             | 21.8     | 22.4    | 19.1          | 22.8            | 50.1     | 18.4       | 25.0  |
| Aquila2-33B-16K           | PI+SFT             |          |         |               |                 |          |            |       |

### æ¨ç†ä»»åŠ¡è¡¨ç°
| Model                                                           | bAbI -task 16 | CLUTRR | bAbI -task 15 | EntailmentBank | aNLI dataset | CommonsenseQA dataset | PiQA dataset | Pep-3K dataset | E-Care dataset | Average     |
|:---------------------------------------------------------------:|:-------------:|:------:|:-------------:|:--------------:|:------------:|:---------------------:|:------------:|:--------------:|:--------------:|:-----------:|
| ChatGPT                                                         | 0.47          | 0.07   | 0.866666667   | 0.833333333    | 0.633333333  | 0.7                   | 0.8          | 0.5            | 0.466666667    | 0.59        |
| GPT4                                                            | 0.93          | 0.37   | 1             | 0.9            | 0.833333333  | 0.833333333           | 0.966666667  | 0.6            | 0.833333333    | 0.81        |
| Baichuan-13B-Chat                                               | 0.73          | 0.03   | 0.17          | 0.57           | 0.6          | 0.6                   | 0.633333333  | 0.6            | 0.466666667    | 0.49        |
| Baichuan2-7B-Chat                                               | 0.4           | 0.27   | 0.43          | 0.73           | 0.53         | 0.63                  | 0.2          | 0.63           | 0.5            | 0.45        |
| Baichuan2-13B-Chat                                              | 0.33          | 0.1    | 0.67          | 0.8            | 0.66         | 0.77                  | 0.6          | 0.53           | 0.63           | 0.57        |
| Qwen-7B-chat                                                    | 0.2           | 0.1    | 0.67          | 0.87           | 0.57         | 0.87                  | 0.37         | 0.63           | 0.57           | 0.54        |
| Qwen-14B-chat                                                   | 0.27          | 0.1    | 0.63          | 0.87           | 0.63         | 0.87                  | 0.7          | 0.77           | 0.57           | 0.6         |
| INternLM-20B-chat                                               | 0.47          | 0.13   | 0.43          | 0.8            | 0.7          | 0.93                  | 0.6          | 0.7            | 0.7            | 0.61        |
| Aquila-33b-knowledge6-135000-megatron-sft-v0.912-ep2            | 0.63          | 0.27   | 0.5           | 0.73           | 0.6          | 0.53                  | 0.46         | 0.5            | 0.7333333      | 0.55        |
| Aquila-33b-knowledge6-341000-sft-v0.9.16                        | 0.55          | 0.17   | 0.53          | 0.77           | 0.8          | 0.7                   | 0.5          | 0.57           | 0.6            | 0.58        |
| Aquila-7b-knowledge5-ep2-110973-chat-clean-coig-pc-ep1-sft-v0.911-ep2-hf | 0.73 | 0.03   | 0.4 | 0.56 | 0.57 | 0.43 | 0.4 | 0.5 | 0.43 | 0.45 |
| Aquila-7b-knowledge5-ep2-110973-v910-ep1-knowledge62-150000-megatron-sft-v0.915-ep3-bsz64-tpl-hf | 0.5 | 0.23 | 0.33 | 0.7 | 0.53 | 0.6 | 0.47 | 0.47 | 0.57 | 0.488888889 |


<br><br>

## å®‰è£…ç¯å¢ƒ

* Python ç‰ˆæœ¬ >= 3.8
* PyTorch ç‰ˆæœ¬ >= 1.8.0
* CUDA ç‰ˆæœ¬ >= 11.7ï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬ä¸ºæ‚¨å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹, æ¥æ¼”ç¤ºå¦‚ä½•å¿«é€Ÿä¸Šæ‰‹Aquila2.

åœ¨æ‚¨åŠ¨æ‰‹æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®è®¤æ‚¨å·²ç»è®¾ç½®å¥½äº†è¿è¡Œç¯å¢ƒï¼Œå¹¶æˆåŠŸå®‰è£…äº†å¿…è¦çš„ä»£ç åŒ…ã€‚é¦–å…ˆï¼Œè¯·ç¡®ä¿æ»¡è¶³è¿™äº›å…ˆå†³æ¡ä»¶ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢çš„æŒ‡ç¤ºå®‰è£…å¿…è¦çš„åº“å’Œä¾èµ–ã€‚

```
pip install -r requirements.txt
```

å¦‚æœæ‚¨çš„æ˜¾å¡å…¼å®¹ fp16 æˆ– bf16 ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜å»ºè®®æ‚¨å®‰è£… flash-attentionï¼Œä»¥å¢åŠ è¿è¡Œé€Ÿåº¦å’Œå‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚è¯·æ³¨æ„ï¼Œflash-attention ä¸æ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒæ‚¨ä¹Ÿèƒ½æ­£å¸¸æ‰§è¡Œè¯¥é¡¹ç›®ã€‚

flash-attentionå®‰è£…ï¼šå‚è€ƒ https://github.com/Dao-AILab/flash-attention/

ç°åœ¨å¯ä»¥å¼€å§‹ä½¿ç”¨ Transformers æˆ– Modelhub æ¥è¿è¡Œæˆ‘ä»¬çš„æ¨¡å‹ã€‚


### ModelHub

è¦ä½¿ç”¨ Aquila2-Chat è¿›è¡Œæ¨ç†ï¼Œä½ åªéœ€è¦è¾“å…¥ä¸‹é¢æ¼”ç¤ºçš„å‡ è¡Œä»£ç ã€‚

```python
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'AquilaChat2-7B'
# model_name = 'AquilaChat2-34B'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_name)
# ä½¿ç”¨model_dirå‚æ•°è°ƒæ•´æ¨¡å‹åŠ è½½è·¯å¾„
# autoloader = AutoLoader("aquila2", model_dir='./checkpoints', model_name=model_name)
# å¦‚éœ€åŠ è½½LoRAæ¨¡å—ï¼Œéœ€è¦é¢å¤–æä¾›LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œlora_dir='./examples/checkpoints/lora/aquila2chat-hf')
# å¦‚éœ€åŠ è½½Q-LoRAæ¨¡å—ï¼Œéœ€è¦é¢å¤–æä¾›Q-LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œqlora_dir='./examples/checkpoints/qlora/aquila2chat-hf')

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()


# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer))
```

æˆ‘ä»¬è¿è¡Œçš„ç»“æœå¦‚ä¸‹:
```
model in: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­###Assistant:
åŒ—äº¬åå¤§æ™¯ç‚¹: 1. å¤©å®‰é—¨å¹¿åœº 2. æ•…å®« 3. é¢å’Œå›­ 4. å¤©å› 5. é¸Ÿå·¢ 6. åŒ—äº¬å¤§å­¦ 7. æ¸…åå¤§å­¦ 8. åŒ—äº¬åŠ¨ç‰©å›­ 9. åŒ—äº¬æ¤ç‰©å›­ 10. é•¿åŸã€‚
model in: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥###Assistant:
çšæ´æœˆå…‰æ´’ä¹æ´²ï¼Œå›¢åœ†ä½³èŠ‚å€æ€æ‚ ã€‚
```

åŸºç¡€æ¨¡å‹æ¨ç†çš„ç”¨æ³•ç±»ä¼¼ï¼Œä¸å¯¹è¯æ¨¡å‹çš„ä¸åŒä¹‹å¤„åªåœ¨äºæ¨¡å‹æ¨ç†çš„æ—¶å€™éœ€è¦è®¾ç½®`sft=False`

<details>
  <summary>Aquila2åŸºç¡€æ¨¡å‹æ¨ç†</summary>

```python
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'Aquila2-7B'
# model_name = 'Aquila2-34B'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_name)

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()

# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer, sft=False))
```

</details>


## é‡åŒ–

### ç”¨æ³•

```python
import torch 
from flagai.auto_model.auto_loader import AutoLoader
from transformers import BitsAndBytesConfig


model_name = 'AquilaChat2-7B'

autoloader = AutoLoader("aquila2", model_name=model_name, 
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    ))

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()
# 

test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
    "Write a tongue twister that's extremely difficult to pronounce.",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer))

```

### æ•ˆæœè¯„æµ‹

---é‡åŒ–æ•ˆæœ(å¯é€‰)---

### æ¨ç†é€Ÿåº¦

---é‡åŒ–æ¨ç†é€Ÿåº¦(å¯é€‰)---

### æ˜¾å­˜ä½¿ç”¨

---é‡åŒ–æ˜¾å­˜ä½¿ç”¨(å¯é€‰)---
<br><br>

## å¾®è°ƒ

æˆ‘ä»¬ä¸ºç”¨æˆ·æä¾›äº†ä¸€ç³»åˆ—å¾®è°ƒè„šæœ¬ï¼Œç”¨äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨è„šæœ¬çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œç”¨æˆ·ä¼šæ‰¾åˆ°è¯¦ç»†çš„è¯´æ˜ï¼ŒæŒ‡æ˜å“ªäº›å‚æ•°éœ€è¦æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

åœ¨è¿›è¡Œå¾®è°ƒæ“ä½œä¹‹å‰ï¼Œæ‚¨å¿…é¡»å…ˆå‡†å¤‡å¥½æ‚¨çš„è®­ç»ƒæ•°æ®ã€‚æ‰€æœ‰æ ·æœ¬éœ€è¦é›†ä¸­åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œå¹¶å­˜å‚¨åœ¨ä¸€ä¸ª json æ–‡ä»¶é‡Œã€‚æ¯ä¸ªæ ·æœ¬åº”è¡¨ç°ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬ id å’Œ conversationï¼Œå…¶ä¸­ï¼Œconversation ä»¥åˆ—è¡¨çš„å½¢å¼å±•ç°ã€‚ä»¥ä¸‹æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ï¼š

```json
{"id": "alpaca_data.json_1", "conversations": [{"from": "human", "value": "What are the three primary colors?"}, {"from": "gpt", "value": "The three primary colors are red, blue, and yellow."}], "instruction": ""}
```

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›ä¸åŒçš„å¾®è°ƒè„šæœ¬å®ç°ä¸åŒåŠŸèƒ½ï¼š
- ä½¿ç”¨`finetune/7B/finetune.sh`å®ç°7Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_lora.sh`å®ç°7Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_qlora.sh`å®ç°7Bæ¨¡å‹Q-LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune.sh`å®ç°34Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_lora.sh`å®ç°34Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_qlora.sh`å®ç°34Bæ¨¡å‹Q-LoRAå¾®è°ƒ 

Note that you are required to specify the path to the training data within the script, and configure the hostfile accordingly. If a custom model file is not provided in the script, it will automatically download the corresponding model from ModelHub based on the specified model name and proceed with the fine-tuning operation.


To perform full-parameter fine-tuning, execute the following scripts:

```bash
# Fine-tuning the 7B model
bash finetune/7B/finetune.sh
# Fine-tuning the 34B model
bash finetune/34B/finetune.sh
```

The fine-tuning approach of LoRA (as detailed in the [paper](https://arxiv.org/abs/2106.09685)) varies from the full-parameter method. LoRA solely updates the parameters of the adapter layer without modifying the original language model parameters. This practice reduces memory and computational overhead. Applicable to a variety of model sizes and tasks, LoRA facilitates more efficient model fine-tuning to cater to specific tasks or datasets.

To implement LoRA, execute the following scripts:

```bash
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_lora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_lora.sh
```

If memory resources remain constrained, consider employing Q-LoRA (refer to the [paper](https://arxiv.org/abs/2305.14314)), an optimized solution that further reduces memory usage through the utilization of 4-bit quantized models and paged attention techniques.

To implement Q-LoRA, execute the following scripts:

```bash
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_qlora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_qlora.sh
```




### ä¼˜åŒ–æ•ˆæœ

ä»¥ä¸‹æ˜¯7Bå’Œ34Bæ¨¡å‹ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼ŒLoRA å’Œ QLoRA å¤„ç†ä¸åŒè¾“å…¥é•¿åº¦æ—¶çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æ•°æ®ã€‚è¯„æµ‹æ˜¯åœ¨ä¸€å°è£…å¤‡æœ‰ A100-SXM4-80G GPU çš„æœºå™¨ä¸Šè¿›è¡Œï¼Œä½¿ç”¨ CUDA 12.1 å’Œ Pytorch 2.1ã€‚å…¶ä¸­7Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º2048ï¼Œ 34Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º4096ã€‚æˆ‘ä»¬è¿›è¡Œçš„æ‰€æœ‰æµ‹è¯•å‡é‡‡ç”¨äº†æ‰¹æ¬¡å¤§å°ä¸º 4 å’Œæ¢¯åº¦ç´¯ç§¯ä¸º 1 çš„é…ç½®ï¼Œå¹¶ä¸”è®°å½•äº†ä»¥GBä¸ºå•ä½çš„æ˜¾å­˜å ç”¨å’Œä»¥s/iterä¸ºå•ä½çš„è®­ç»ƒé€Ÿåº¦ã€‚å…·ä½“çš„æ•°æ®å¦‚ä¸‹ï¼š

<table>
    <tr>
      <th>æ¨¡å‹å¤§å°</th><th>å¾®è°ƒæ–¹æ³•</th><th>æ˜¾å­˜å ç”¨</th><th>è®­ç»ƒé€Ÿåº¦</th>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td>SFT</td><td>43.9G</td><td>2.67s/iter</td>
    </tr>
    <tr>
        <td>LoRA</td><td>29.4G</td><td>2.04s/iter</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td>19.9G</td><td>2.14s/iter</td>
    </tr>
    <tr>
        <th rowspan="1">34B</th><td>Q-LoRA</td><td>37.7G</td><td>8.22s/iter</td>
    </tr>
</table>



<br><br>

## é¢„è®­ç»ƒ

---é¢„è®­ç»ƒä½¿ç”¨(ç‰é¾™)---
<br><br>

## é•¿æ–‡æœ¬ç†è§£

---ä»‹ç»---

---è¯„æµ‹ç»“æœ---

## Tokenization

---ä¸­æ–‡å¯ä»¥ç®€å•è¯´è¯´tokenizationæ˜¯ä»€ä¹ˆï¼ˆå› ä¸ºè¿™è¯æ²¡æœ‰å¥½çš„ä¸­æ–‡å¯¹åº”ç¿»è¯‘ï¼‰---

---ç»™ä¸€ä¸ªtokenizeræ–‡æ¡£çš„link(å¯é€‰)---
<br><br>

## å¤ç°

---å¤ç°è¯„æµ‹çš„è„šæœ¬(å¯é€‰)---
<br><br>

## FAQ

æ¬¢è¿åœ¨ [GitHub Issues](https://github.com/FlagAI-Open/FlagAI/issues) ä¸­æå‡ºä½ çš„é—®é¢˜ï¼Œæˆ–åœ¨ [Discussions ](https://github.com/FlagAI-Open/FlagAI/discussions) æ¿å—äº¤æµä½¿ç”¨ç»éªŒã€‚

---ä¹‹åå¯ä»¥å¼„ä¸€ä¸ªå¸¸è§é—®é¢˜çš„æ–‡æ¡£linkæ”¾åˆ°è¿™é‡Œ---
<br><br>

## ä½¿ç”¨åè®®

FlagAIé£æ™ºå¤§éƒ¨åˆ†é¡¹ç›®åŸºäº [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)
---å¯èƒ½è¿˜éœ€è¦è¡¥å……---
<br><br>

## è”ç³»æˆ‘ä»¬

* å®˜æ–¹é‚®ç®±ï¼šopen.platform@baai.ac.cnã€‚
* çŸ¥ä¹ï¼š[FlagAIé£æ™º](https://www.zhihu.com/people/95-22-20-18)
* æ‰«ç æ·»åŠ å°åŠ©æ‰‹åŠ å…¥**å¾®ä¿¡äº¤æµç¾¤**ï¼š

<img src="./assets/wechat-qrcode.jpg" width = "200" height = "200"  align=center />

