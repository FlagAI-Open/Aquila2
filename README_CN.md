<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
    <img src="./assets/logo.png" width="500"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/BAAI">Hugging Face</a>&nbsp&nbsp | &nbsp <a href="https://model.baai.ac.cn/models">ModelHub</a>&nbsp&nbsp | &nbsp&nbsp <a href="assets/wechat-qrcode.png">å¾®ä¿¡</a>
</p>
<br><br>

æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„ **Aquila2** ç³»åˆ—ï¼Œç°åœ¨åŒ…æ‹¬åŸºç¡€è¯­è¨€æ¨¡å‹ **Aquila2-7B** å’Œ **Aquila2-34B** ï¼Œä»¥åŠå¯¹è¯æ¨¡å‹ **Aquila2-7B-Chat** å’Œ **Aquila2-34B-Chat**ã€‚

| æ¨¡å‹åç§°         | Modelhub  | Huggingface | 
|----------------------|:----:|:-----------: |
| Aquila2-7B | https://model.baai.ac.cn/model-detail/100118 |    -     | 
| AquilaChat2-7B | https://model.baai.ac.cn/model-detail/100117 |   -      | 
| Aquila2-34B | https://model.baai.ac.cn/model-detail/100119  |    -    | 
| AquilaChat2-34B | https://model.baai.ac.cn/model-detail/100116 |   -      |

åœ¨è¿™ä¸ªä»“åº“ä¸­ï¼Œæ‚¨å¯ä»¥ï¼š

* å¿«é€Ÿå¼€å§‹ä½¿ç”¨ Aquila2ï¼Œè¿›è¡Œç®€å•çš„æ¨ç†ã€‚
* æœ‰å…³é‡åŒ–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ–¹æ³•ã€å†…å­˜ã€æ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†æ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº† BF16 æ¨¡å‹çš„ç»Ÿè®¡æ•°æ®ã€‚
* å¾®è°ƒæ•™ç¨‹ï¼ŒåŒ…æ‹¬å…¨å‚æ•°è°ƒä¼˜ã€LoRA å’Œ Q-LoRAã€‚
* é•¿æ–‡æœ¬ç†è§£è¯„ä¼°çš„ç»Ÿè®¡æ•°æ®
* è®¸å¯åè®®
* ...

æ¬¢è¿å¯¹æˆ‘ä»¬æå‡ºä»»ä½•é—®é¢˜ï¼ˆå»ºè®®ç”¨è‹±è¯­ï¼Œè¿™æ ·æ›´å¤šäººä¼šæ˜ç™½ä½ çš„é—®é¢˜å“¦ï¼‰ï¼å¦‚æœæœ‰å…´è¶£å¸®æˆ‘ä»¬æ”¹è¿› **Aquila2**ï¼Œå¯ä»¥æäº¤ä½ çš„Pull Requestsï¼Œ æˆ‘ä»¬ä¼šåŠæ—¶å¤„ç†ã€‚

å¦‚æœä½ æƒ³ä¸æˆ‘ä»¬è¿›è¡Œè®¨è®ºå’Œäº¤æµï¼Œè¯·å°½å¿«åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å§(è¯·å‚è§æ–‡æ¡£é¡¶éƒ¨ä»¥è·å–å…¥å£ä¿¡æ¯)ï¼



<br>

## æ›´æ–°

* 2023.10.10 ğŸ”¥ æˆ‘ä»¬åœ¨ ModelHub å’Œ Hugging Face ä¸Šå‘å¸ƒäº† **Aquila2-34B** å’Œ **Aquila2-34B-Chat**ã€‚

## è¯„æµ‹è¡¨ç°

Aquila2-34Bå’ŒAquila2-7Bï¼ˆæœ€æ–°ç‰ˆæœ¬ä½¿ç”¨äº†æ›´å¤šæ•°æ®å’Œæ›´é•¿çš„ä¸Šä¸‹æ–‡è¿›è¡Œäº†è®­ç»ƒï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä»2048æ‰©å±•åˆ°äº†8192ï¼‰ç›¸æ¯”åŒè§„æ¨¡çš„åŸºçº¿æ¨¡å‹åœ¨å„é¡¹è¯„æµ‹æ•°æ®é›†ä¸Šå‡è¡¨ç°æ›´ä¼˜ï¼Œè¯„æµ‹æ•°æ®é›†åŒ…æ‹¬MMLUã€C-Evalã€GSM8Kã€MATHã€HumanEvalç­‰ï¼Œè€ƒå¯Ÿäº†æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€æ•°å­¦é—®é¢˜æ±‚è§£èƒ½åŠ›ã€ä»£ç èƒ½åŠ›ç­‰å„æ–¹é¢èƒ½åŠ›ã€‚

### åŸºç¡€æ¨¡å‹è¡¨ç°

|      Model      | C-Eval |  MMLU  | CMMLU  | GSM8K  | GaoKao |  MATH  | HumanEval | WMT22 (en-zh) | WinoGrande |
| :-------------: | :----: | :----: | :----: | :----: | :----: | :----: | :-------: | :-----------: | :--------: |
|                 | 5-shot | 5-shot | 5-shot | 8-shot |        | 4-shot |  0-shot   |    0-shot     |   0-shot   |
|   InternLM-7B   |  48.6  |  51.2  |  51.8  |  31.2  |  49.6  |  6.3   |   13.4    |     53.3      |    68.2    |
|  InternLM-20B   |  53.7  |  61.8  |  59.0  |  52.6  |  63.6  |  7.9   |   25.6    |     56.9      |    75.1    |
|   ChatGLM2-6B   |  51.7  |  47.9  |  48.8  |  32.4  |  49.4  |  6.5   |    9.2    |     45.7      |            |
|  ChatGLM2-12B   |  61.6  |  56.2  |        |  40.9  |        |        |           |               |            |
|  Baichuan2-7B   |  52.3  |  54.6  |  57.1  |  24.5  |  53.6  |  5.6   |   18.3    |     55.9      |    68.4    |
|  Baichuan2-13B  |  55.6  |  56.9  |  62.0  |  52.8  |  59.7  |  10.1  |   17.1    |     60.5      |    70.3    |
|     Qwen-7b     |  56.7  |  58.0  |  62.2  |  51.7  |  58.5  |  6.5   |   29.9    |     58.1      |    66.1    |
|    Qwen-14b     |  71.4  |  65.8  |  70.5  |  58.7  |  65.4  |  13.4  |   32.3    |     55.0      |    67.4    |
|    LLaMA2-7B    |  34.1  |  46.9  |  31.4  |  16.2  |  41.7  |  3.2   |   12.8    |     36.4      |    67.1    |
|   LLaMA2-70B    |  52.1  |  69.5  |        |  56.8  |  64.5  |  13.5  |   29.9    |               |    78.0    |
| **Aquila2-7B**  |  48.9  |  54.9  |  56.1  |  41.9  |  54.0  |  10.9  |   21.4    |     57.3      |    67.5    |
| **Aquila2-33B** |  62.2  |  60.0  |  65.9  |  56.3  |  64.6  |  11.6  |   25.3    |     60.0      |    70.6    |

<br>

### å¯¹è¯æ¨¡å‹è¡¨ç°

|      Model          | Placeholder |
| :-----------------: | :---------: |
| **AquilaChat2-7B**  |             |
| **AquilaChat2-33B** |             |

<br>

### é•¿æ–‡æœ¬ä»»åŠ¡è¡¨ç°
|           Model           |       Method       | SingleQA | MultiQA | Summarization | Code Completion | Few Shot | Synthetics | Selection | Other |
| :-----------------------: | :----------------: | :------: | :-----: | :-----------: | :-------------: | :------: | :--------: | :-------: | :---: |
|     GPT-3.5-Turbo-16K     |      Unknown       |   47.6   |  36.2   |     23.0      |      54.5       |   77.5   |    27.5    |   33.6    | 35.3  |
|   LongChat-7B-v1.5-32K    |       PI+SFT       |   27.5   |  20.3   |     22.5      |      57.0       |   62.9   |    18.8    |   21.7    | 23.7  |
|      ChatGLM2-6B-32K      | Continual Pretrain |   44.1   |  34.7   |     20.8      |      52.7       |   68.7   |    23.6    |   30.8    | 31.6  |
|  Baichuan2-13B-Chat-16K   |        None        |   14.5   |  12.6   |     14.0      |                 |   22.2   |    11.9    |   11.6    | 14.7  |
| Qwen-14B-Chat-dynamic-ntk |    Dynamic NTK     |   24.4   |  20.2   |     22.3      |      37.3       |   47.2   |    18.6    |   16.1    | 24.0  |
|     Internlm-20B-Chat     |        None        |   19.2   |  17.5   |     16.7      |      26.0       |   41.0   |    16.5    |   16.6    | 19.7  |
|    **Aquila2-7B-16K**     |       PI+SFT       |   21.8   |  22.4   |     19.1      |      22.8       |   50.1   |    18.4    |   29.5    | 25.0  |
|    **Aquila2-33B-16K**    |       PI+SFT       |          |         |               |                 |          |            |   31.7    | 29.5  |

<br>

### æ¨ç†ä»»åŠ¡è¡¨ç°

| Model                        | bAbI#16<br>(Inductive) | CLUTRR<br>(Inductive) | bAbI#15<br>(Deductive) | EntailmentBank<br>(Deductive) | Î±NLI<br>(Abductive) | E-Care<br>(Casual) | Avg. |
| :--------------------------- | :--------------------: | :-------------------: | :--------------------: | :---------------------------: | :-----------------: | :----------------: | :--: |
| Baichuan2-7B-Chat            |          40.0          |         26.7          |          43.3          |             73.3              |        53.3         |        50.0        | 47.8 |
| Qwen-7B-Chat                 |          20.0          |         10.0          |          66.7          |             86.7              |        56.7         |        56.7        | 49.5 |
| Qwen-14B-Chat                |          26.7          |         10.0          |          63.3          |             86.7              |        63.3         |        56.7        | 51.1 |
| Baichuan2-13B-Chat           |          33.3          |         10.0          |          66.7          |             80.0              |        66.7         |        63.3        | 53.3 |
| InternLM-20B-Chat            |          46.7          |         13.3          |          43.3          |             80.0              |        70.0         |        70.0        | 53.9 |
| ChatGPT                      |          46.7          |          6.7          |          86.7          |             83.3              |        63.3         |        46.7        | 55.6 |
| LLaMA-70B-Chat               |          63.3          |         20.0          |          53.3          |             80.0              |        66.7         |        60.0        | 57.2 |
| GPT-4                        |          93.3          |         36.7          |         100.0          |             90.0              |        83.3         |        83.3        | 81.1 |
| **Aquila2-34B-Chat**         |          43.3          |         16.7          |          63.6          |             80.0              |        80.0         |        66.7        | 58.3 |
| **Aquila2-34B-Chat+SFT**     |          73.3          |         16.7          |          76.7          |             80.0              |        76.7         |        70.0        | 65.6 |
| **Aquila2-34B-Chat+SFT+CoT** |          80.0          |         23.3          |          83.3          |             73.3              |        80.0         |        76.7        | 69.4 |

<br>

## å®‰è£…ç¯å¢ƒ

* Python ç‰ˆæœ¬ >= 3.8
* PyTorch ç‰ˆæœ¬ >= 1.8.0
* CUDA ç‰ˆæœ¬ >= 11.7ï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬ä¸ºæ‚¨å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹, æ¥æ¼”ç¤ºå¦‚ä½•å¿«é€Ÿä¸Šæ‰‹Aquila2.

åœ¨æ‚¨åŠ¨æ‰‹æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®è®¤æ‚¨å·²ç»è®¾ç½®å¥½äº†è¿è¡Œç¯å¢ƒï¼Œå¹¶æˆåŠŸå®‰è£…äº†å¿…è¦çš„ä»£ç åŒ…ã€‚é¦–å…ˆï¼Œè¯·ç¡®ä¿æ»¡è¶³è¿™äº›å…ˆå†³æ¡ä»¶ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢çš„æŒ‡ç¤ºå®‰è£…å¿…è¦çš„åº“å’Œä¾èµ–ã€‚

```
pip install -r requirements.txt
```

å¦‚æœæ‚¨çš„æ˜¾å¡å…¼å®¹ fp16 æˆ– bf16 ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜å»ºè®®æ‚¨å®‰è£… flash-attentionï¼Œä»¥å¢åŠ è¿è¡Œé€Ÿåº¦å’Œå‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚è¯·æ³¨æ„ï¼Œflash-attention ä¸æ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒæ‚¨ä¹Ÿèƒ½æ­£å¸¸æ‰§è¡Œè¯¥é¡¹ç›®ã€‚

flash-attentionå®‰è£…ï¼šå‚è€ƒ https://github.com/Dao-AILab/flash-attention/

ç°åœ¨å¯ä»¥å¼€å§‹ä½¿ç”¨ Transformers æˆ– Modelhub æ¥è¿è¡Œæˆ‘ä»¬çš„æ¨¡å‹ã€‚


### ModelHub

è¦ä½¿ç”¨ Aquila2-Chat è¿›è¡Œæ¨ç†ï¼Œä½ åªéœ€è¦è¾“å…¥ä¸‹é¢æ¼”ç¤ºçš„å‡ è¡Œä»£ç ã€‚

```python
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'AquilaChat2-7B'
# model_name = 'AquilaChat2-34B'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_name)
# ä½¿ç”¨model_dirå‚æ•°è°ƒæ•´æ¨¡å‹åŠ è½½è·¯å¾„
# autoloader = AutoLoader("aquila2", model_dir='./checkpoints', model_name=model_name)
# å¦‚éœ€åŠ è½½LoRAæ¨¡å—ï¼Œéœ€è¦é¢å¤–æä¾›LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œlora_dir='./examples/checkpoints/lora/aquila2chat-hf')
# å¦‚éœ€åŠ è½½Q-LoRAæ¨¡å—ï¼Œéœ€è¦é¢å¤–æä¾›Q-LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œqlora_dir='./examples/checkpoints/qlora/aquila2chat-hf')

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()


# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer))
```

æˆ‘ä»¬è¿è¡Œçš„ç»“æœå¦‚ä¸‹:
```
åŒ—äº¬åå¤§æ™¯ç‚¹: 1. å¤©å®‰é—¨å¹¿åœº 2. æ•…å®« 3. é¢å’Œå›­ 4. å¤©å› 5. é¸Ÿå·¢ 6. åŒ—äº¬å¤§å­¦ 7. æ¸…åå¤§å­¦ 8. åŒ—äº¬åŠ¨ç‰©å›­ 9. åŒ—äº¬æ¤ç‰©å›­ 10. é•¿åŸã€‚

çšæ´æœˆå…‰æ´’ä¹æ´²ï¼Œå›¢åœ†ä½³èŠ‚å€æ€æ‚ ã€‚
```

åŸºç¡€æ¨¡å‹æ¨ç†çš„ç”¨æ³•ç±»ä¼¼ï¼Œä¸å¯¹è¯æ¨¡å‹çš„ä¸åŒä¹‹å¤„åªåœ¨äºæ¨¡å‹æ¨ç†çš„æ—¶å€™éœ€è¦è®¾ç½®`sft=False`

<details>
  <summary>Aquila2åŸºç¡€æ¨¡å‹æ¨ç†</summary>

```python
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'Aquila2-7B'
# model_name = 'Aquila2-34B'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_name)

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()

# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer, sft=False))
```

</details>


## é‡åŒ–

### ç”¨æ³•

```python
import torch 
from flagai.auto_model.auto_loader import AutoLoader
from transformers import BitsAndBytesConfig


model_name = 'AquilaChat2-7B'

autoloader = AutoLoader("aquila2", model_name=model_name, 
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    ))

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()
# 

test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
    "Write a tongue twister that's extremely difficult to pronounce.",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer))

```

### æ•ˆæœè¯„æµ‹

---é‡åŒ–æ•ˆæœ(å¯é€‰)---

### æ¨ç†é€Ÿåº¦

---é‡åŒ–æ¨ç†é€Ÿåº¦(å¯é€‰)---

### æ˜¾å­˜ä½¿ç”¨

---é‡åŒ–æ˜¾å­˜ä½¿ç”¨(å¯é€‰)---
<br><br>

## å¾®è°ƒ

æˆ‘ä»¬ä¸ºç”¨æˆ·æä¾›äº†ä¸€ç³»åˆ—å¾®è°ƒè„šæœ¬ï¼Œç”¨äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨è„šæœ¬çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œç”¨æˆ·ä¼šæ‰¾åˆ°è¯¦ç»†çš„è¯´æ˜ï¼ŒæŒ‡æ˜å“ªäº›å‚æ•°éœ€è¦æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

åœ¨è¿›è¡Œå¾®è°ƒæ“ä½œä¹‹å‰ï¼Œæ‚¨å¿…é¡»å…ˆå‡†å¤‡å¥½æ‚¨çš„è®­ç»ƒæ•°æ®ã€‚æ‰€æœ‰æ ·æœ¬éœ€è¦é›†ä¸­åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œå¹¶å­˜å‚¨åœ¨ä¸€ä¸ª json æ–‡ä»¶é‡Œã€‚æ¯ä¸ªæ ·æœ¬åº”è¡¨ç°ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬ id å’Œ conversationï¼Œå…¶ä¸­ï¼Œconversation ä»¥åˆ—è¡¨çš„å½¢å¼å±•ç°ã€‚ä»¥ä¸‹æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ï¼š

```json
{"id": "alpaca_data.json_1", "conversations": [{"from": "human", "value": "What are the three primary colors?"}, {"from": "gpt", "value": "The three primary colors are red, blue, and yellow."}], "instruction": ""}
```

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›ä¸åŒçš„å¾®è°ƒè„šæœ¬å®ç°ä¸åŒåŠŸèƒ½ï¼š
- ä½¿ç”¨`finetune/7B/finetune.sh`å®ç°7Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_lora.sh`å®ç°7Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_qlora.sh`å®ç°7Bæ¨¡å‹Q-LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune.sh`å®ç°34Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_lora.sh`å®ç°34Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_qlora.sh`å®ç°34Bæ¨¡å‹Q-LoRAå¾®è°ƒ 

Note that you are required to specify the path to the training data within the script, and configure the hostfile accordingly. If a custom model file is not provided in the script, it will automatically download the corresponding model from ModelHub based on the specified model name and proceed with the fine-tuning operation.


To perform full-parameter fine-tuning, execute the following scripts:

```bash
# Fine-tuning the 7B model
bash finetune/7B/finetune.sh
# Fine-tuning the 34B model
bash finetune/34B/finetune.sh
```

The fine-tuning approach of LoRA (as detailed in the [paper](https://arxiv.org/abs/2106.09685)) varies from the full-parameter method. LoRA solely updates the parameters of the adapter layer without modifying the original language model parameters. This practice reduces memory and computational overhead. Applicable to a variety of model sizes and tasks, LoRA facilitates more efficient model fine-tuning to cater to specific tasks or datasets.

To implement LoRA, execute the following scripts:

```bash
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_lora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_lora.sh
```

If memory resources remain constrained, consider employing Q-LoRA (refer to the [paper](https://arxiv.org/abs/2305.14314)), an optimized solution that further reduces memory usage through the utilization of 4-bit quantized models and paged attention techniques.

To implement Q-LoRA, execute the following scripts:

```bash
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_qlora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_qlora.sh
```




### ä¼˜åŒ–æ•ˆæœ

ä»¥ä¸‹æ˜¯7Bå’Œ34Bæ¨¡å‹ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼ŒLoRA å’Œ QLoRA å¤„ç†ä¸åŒè¾“å…¥é•¿åº¦æ—¶çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æ•°æ®ã€‚è¯„æµ‹æ˜¯åœ¨ä¸€å°è£…å¤‡æœ‰ A100-SXM4-80G GPU çš„æœºå™¨ä¸Šè¿›è¡Œï¼Œä½¿ç”¨ CUDA 12.1 å’Œ Pytorch 2.1ã€‚å…¶ä¸­7Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º2048ï¼Œ 34Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º4096ã€‚æˆ‘ä»¬è¿›è¡Œçš„æ‰€æœ‰æµ‹è¯•å‡é‡‡ç”¨äº†æ‰¹æ¬¡å¤§å°ä¸º 4 å’Œæ¢¯åº¦ç´¯ç§¯ä¸º 1 çš„é…ç½®ï¼Œå¹¶ä¸”è®°å½•äº†ä»¥GBä¸ºå•ä½çš„æ˜¾å­˜å ç”¨å’Œä»¥s/iterä¸ºå•ä½çš„è®­ç»ƒé€Ÿåº¦ã€‚å…·ä½“çš„æ•°æ®å¦‚ä¸‹ï¼š

<table>
    <tr>
      <th>æ¨¡å‹å¤§å°</th><th>å¾®è°ƒæ–¹æ³•</th><th>æ˜¾å­˜å ç”¨</th><th>è®­ç»ƒé€Ÿåº¦</th>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td>SFT</td><td>43.9G</td><td>2.67s/iter</td>
    </tr>
    <tr>
        <td>LoRA</td><td>29.4G</td><td>2.04s/iter</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td>19.9G</td><td>2.14s/iter</td>
    </tr>
    <tr>
        <th rowspan="1">34B</th><td>Q-LoRA</td><td>37.7G</td><td>8.22s/iter</td>
    </tr>
</table>



<br><br>

## é¢„è®­ç»ƒ

---é¢„è®­ç»ƒä½¿ç”¨(ç‰é¾™)---
<br><br>

## é•¿æ–‡æœ¬ç†è§£

---ä»‹ç»---

---è¯„æµ‹ç»“æœ---

## Tokenization

---ä¸­æ–‡å¯ä»¥ç®€å•è¯´è¯´tokenizationæ˜¯ä»€ä¹ˆï¼ˆå› ä¸ºè¿™è¯æ²¡æœ‰å¥½çš„ä¸­æ–‡å¯¹åº”ç¿»è¯‘ï¼‰---

---ç»™ä¸€ä¸ªtokenizeræ–‡æ¡£çš„link(å¯é€‰)---
<br><br>

## å¤ç°

---å¤ç°è¯„æµ‹çš„è„šæœ¬(å¯é€‰)---
<br><br>

## FAQ

æ¬¢è¿åœ¨ [GitHub Issues](https://github.com/FlagAI-Open/FlagAI/issues) ä¸­æå‡ºä½ çš„é—®é¢˜ï¼Œæˆ–åœ¨ [Discussions ](https://github.com/FlagAI-Open/FlagAI/discussions) æ¿å—äº¤æµä½¿ç”¨ç»éªŒã€‚

---ä¹‹åå¯ä»¥å¼„ä¸€ä¸ªå¸¸è§é—®é¢˜çš„æ–‡æ¡£linkæ”¾åˆ°è¿™é‡Œ---
<br><br>

## ä½¿ç”¨åè®®

Aquila2é¡¹ç›®åŸºäº [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)
---å¯èƒ½è¿˜éœ€è¦è¡¥å……---
<br><br>

## è”ç³»æˆ‘ä»¬

* å®˜æ–¹é‚®ç®±ï¼šopen.platform@baai.ac.cnã€‚
* çŸ¥ä¹ï¼š[FlagAIé£æ™º](https://www.zhihu.com/people/95-22-20-18)
* æ‰«ç æ·»åŠ å°åŠ©æ‰‹åŠ å…¥**å¾®ä¿¡äº¤æµç¾¤**ï¼š

<img src="./assets/wechat-qrcode.jpg" width = "200" height = "200"  align=center />

