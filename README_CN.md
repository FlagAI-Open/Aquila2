<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
    <img src="./assets/logo.png" width="500"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/BAAI">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp <a href="https://model.baai.ac.cn/models"><img src="assets/baai.png" width="18"/> BAAI ModelHub</a>&nbsp&nbsp | &nbsp&nbsp <a href="assets/wechat-qrcode.jpg">å¾®ä¿¡</a>&nbsp&nbsp |  &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/BAAI">ModelScope</a> &nbsp&nbsp| &nbsp&nbspğŸ§ <a href="https://www.wisemodel.cn/organization/BAAI">WiseModel</a>
</p>
<br><br>

æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„ **Aquila2** ç³»åˆ—ï¼Œç°åœ¨åŒ…æ‹¬åŸºç¡€è¯­è¨€æ¨¡å‹ **Aquila2-7B**ï¼Œ**Aquila2-34B** å’Œ **Aquila2-70B-Expr** ï¼Œå¯¹è¯æ¨¡å‹ **AquilaChat2-7B** ï¼Œ**AquilaChat2-34B** å’Œ **AquilaChat2-70B-Expr**ï¼Œé•¿æ–‡æœ¬å¯¹è¯æ¨¡å‹**AquilaChat2-7B-16k** å’Œ **AquilaChat2-34B-16k**ï¼Œæ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡»ä¸‹æ–¹å›¾æ ‡è¿›å…¥ä¸‹è½½ç•Œé¢ï¼š

| æ¨¡å‹åç§°           |                                                                       ä¸‹è½½æ–¹å¼                                                                        |
|-------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------:|
| Aquila2-7B        |       [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/100118) [ğŸ¤—](https://huggingface.co/BAAI/Aquila2-7B)        |  
| AquilaChat2-7B    |     [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/100117) [ğŸ¤—](https://huggingface.co/BAAI/AquilaChat2-7B)      | 
| AquilaChat2-7B-16k    |   [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/100120) [ğŸ¤—](https://huggingface.co/BAAI/AquilaChat2-7B-16K)    |  
| Aquila2-34B       | [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/100119) [ğŸ¤—](https://huggingface.co/BAAI/AquilaChat2-34B) ğŸ¤–  ğŸ§   | 
| AquilaChat2-34B   | [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/100116) [ğŸ¤—](https://huggingface.co/BAAI/AquilaChat2-34B)  ğŸ¤– ğŸ§   | 
| AquilaChat2-34B-16k    | [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/100121) [ğŸ¤—](https://huggingface.co/BAAI/AquilaChat2-34B-16K) ğŸ¤– ğŸ§  | 
| AquilaChat2-34B-Int4-GPTQ    |  [ğŸ¤–](https://modelscope.cn/models/BAAI/AquilaChat2-34B-Int4-GPTQ/summary)  [ğŸ§ ](https://www.wisemodel.cn/models/BAAI/AquilaChat2-34B-Int4-GPTQ/intro) | 
| Aquila2-70B-Expr        |                                                     [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/220120) [ğŸ¤—](https://huggingface.co/BAAI/Aquila2-70B-Expr)                                                      |  
| AquilaChat2-70B-Expr    |                                                   [<img src="assets/baai.png" width="18"/>](https://model.baai.ac.cn/model-detail/220121) [ğŸ¤—](https://huggingface.co/BAAI/AquilaChat2-70B-Expr)                                                    | 


åœ¨è¿™ä¸ªä»“åº“ä¸­ï¼Œæ‚¨å¯ä»¥ï¼š

* å¿«é€Ÿå¼€å§‹ä½¿ç”¨ Aquila2ï¼Œè¿›è¡Œç®€å•çš„æ¨ç†ã€‚
* æœ‰å…³é‡åŒ–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ–¹æ³•ã€å†…å­˜ã€æ¨ç†é€Ÿåº¦ã€‚
* å¾®è°ƒæ•™ç¨‹ï¼ŒåŒ…æ‹¬å…¨å‚æ•°ã€LoRA å’Œ Q-LoRAã€‚
* é•¿æ–‡æœ¬ç†è§£ä¸è¯„ä¼°
* è®¸å¯åè®®

æ¬¢è¿å¯¹æˆ‘ä»¬æå‡ºä»»ä½•é—®é¢˜ï¼ˆå»ºè®®ç”¨è‹±è¯­ï¼Œè¿™æ ·æ›´å¤šäººä¼šæ˜ç™½ä½ çš„é—®é¢˜å“¦ï¼‰ï¼å¦‚æœæœ‰å…´è¶£å¸®æˆ‘ä»¬æ”¹è¿› **Aquila2**ï¼Œå¯ä»¥æäº¤ä½ çš„Pull Requestsï¼Œ æˆ‘ä»¬ä¼šåŠæ—¶å¤„ç†ã€‚

å¦‚æœæƒ³ä¸æˆ‘ä»¬è¿›è¡Œè®¨è®ºå’Œäº¤æµï¼Œè¯·å°½å¿«åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å§(è¯·å‚è§æ–‡æ¡£é¡¶éƒ¨ä»¥è·å–å…¥å£ä¿¡æ¯)ï¼

<br>

## æ›´æ–°

* 2023.11.30 ğŸ”¥ æˆ‘ä»¬åœ¨ ModelHub å’Œ Hugging Face ä¸Šå‘å¸ƒäº† 70B æ¨¡å‹å®éªŒç‰ˆæœ¬, **Aquila2-70B-Expr** å’Œ **AquilaChat2-70B-Expr**ã€‚

* 2023.11.10 ğŸ”¥ åŸºäºBAAIå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAquila2ï¼‰å’ŒåµŒå…¥æ¨¡å‹ï¼ˆBGEï¼‰ï¼Œåˆ©ç”¨langchainæ„å»ºä¸€ä¸ªåŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„é—®ç­”åº”ç”¨è§£å†³æ–¹æ¡ˆ[Aquila_BGE_langchain](./examples/Aquila_BGE_langchain)ã€‚

* 2023.10.25 ğŸ”¥ 1.2ç‰ˆæœ¬çš„ **Aquila2-34B**, **AquilaChat2-34B-16K** å’Œ **AquilaChat2-34B** æ¨¡å‹å·²åœ¨ModelHub å’Œ Hugging Face ä¸Šæ›´æ–°ã€‚
å…¶ä¸­Aquila2-34Bæ¨¡å‹ç»¼åˆå®¢è§‚è¯„æµ‹æå‡ 6.9%ï¼ŒAquila2-34B v1.2  åœ¨ MMLUã€TruthfulQAã€CSLã€TNEWSã€OCNLIã€BUSTM ç­‰è€ƒè¯•ã€ç†è§£åŠæ¨ç†è¯„æµ‹æ•°æ®é›†ä¸Šçš„è¯„æµ‹ç»“æœåˆ†åˆ«å¢åŠ  12%ã€14%ã€11%ã€12%ã€28%ã€18%ã€‚Chatæ¨¡å‹åœ¨ä¸»è§‚è¯„æµ‹çš„8ä¸ªäºŒçº§èƒ½åŠ›ç»´åº¦ä¸Šï¼Œå‡æ¥è¿‘æˆ–è¶…è¿‡ GPT3.5 æ°´å¹³ã€‚ AquilaChat2-34B-16K-V1.2åˆ™ç›¸è¾ƒäºV1ç‰ˆæœ¬åœ¨é•¿æ–‡æœ¬ç»¼åˆèƒ½åŠ›ä¸Šæœ‰æ˜æ˜¾æå‡ï¼Œæ¥è¿‘GPT-3.5-16Kã€‚

* 2023.10.12 ğŸ”¥ æˆ‘ä»¬åœ¨ ModelHub å’Œ Hugging Face ä¸Šå‘å¸ƒäº† **Aquila2-34B** å’Œ **AquilaChat2-34B**ã€‚

## è¯„æµ‹è¡¨ç°

Aquila2-34Bå’ŒAquila2-7Bç›¸æ¯”åŒè§„æ¨¡çš„åŸºçº¿æ¨¡å‹åœ¨å„é¡¹è¯„æµ‹æ•°æ®é›†ä¸Šå‡è¡¨ç°æ›´ä¼˜ã€‚

### åŸºç¡€æ¨¡å‹è¡¨ç°
<br>
<p align="center">
    <img src="assets/base_metrics_CN.jpeg" width="1024"/>
<p>
<br>
<p>
æ³¨ï¼šå‘ç°åœ¨é¢„è®­ç»ƒä»»åŠ¡æ•°æ®é›†ä¸­å­˜åœ¨GSM8Kæµ‹è¯•æ•°æ®æ³„éœ²é—®é¢˜ï¼Œä»è¯„æµ‹ç»“æœä¸­åˆ é™¤GSM8Kçš„è¯„ä¼°ç»“æœã€‚

ç»å½»æŸ¥åˆ†æï¼Œæ•°æ®æ³„éœ²å‘ç”ŸäºæŸå¤šæ¬¡åˆä½œæ•°æ®å›¢é˜Ÿæ‰€æ¨èçš„æ•°å­¦æ•°æ®é›†Aï¼ˆè¶…è¿‡2ç™¾ä¸‡æ ·æœ¬ï¼‰ï¼Œå…¶åŒ…å«æœªç»è¿‡å¤„ç†çš„GSM8Kæµ‹è¯•é›†ï¼ˆ1319æ ·æœ¬ï¼‰ã€‚å›¢é˜Ÿåªè¿›è¡Œäº†å¸¸è§„å»é‡å’Œè´¨é‡æ£€æµ‹ï¼Œæœªå°±æ˜¯å¦æ··å…¥GSM8Kæµ‹è¯•æ•°æ®è¿›è¡Œé¢å¤–è¿‡æ»¤æ£€æŸ¥è€Œå¯¼è‡´å¤±è¯¯ï¼Œå®ä¸ºå·¥ä½œä¸­çš„ç–æ¼ã€‚

å›¢é˜Ÿä¸€ç›´ä¸¥æ ¼éµå¾ªè®­ç»ƒæ•°æ®ä¸èƒ½åŒ…å«æµ‹è¯•æ•°æ®çš„å·¥ä½œåŸåˆ™ã€‚æ±²å–æœ¬æ¬¡å› æœªå¯¹å¤–éƒ¨æ•°æ®æ¥æºè¿›è¡ŒæŸ¥è¯è€Œå‘ç”Ÿçš„å¤±è¯¯æ•™è®­ï¼Œæˆ‘ä»¬åœ¨2ä¸‡äº¿tokenå…¨é‡æ•°æ®ä¸Šå®Œæˆäº†é’ˆå¯¹21ä¸ªæµ‹è¯•æ•°æ®é›†çš„æ’æŸ¥ï¼Œæ‰€æ¶‰æ•°æ®é›†åŒ…æ‹¬WTM22ï¼ˆen-zhï¼‰ã€CLUEWSCã€winogradeã€HellaSwagã€OpenBookQAã€PIQAã€ARC-eã€BUSTSMã€BoolQã€TruthfulQAã€RAFTã€ChIDã€EPRSTMTã€TNEWSã€OCNLIã€SEM-Chineseã€MMLUã€C-Evalã€CMMLUã€CSLå’ŒHumanEvalã€‚

</p>
<p>
    <b>è¯„æµ‹è¯´æ˜ï¼š</b>
    <br>
    å¯¹äºç”Ÿæˆå¼å¯¹è¯æ¨¡å‹ï¼Œæ™ºæºå›¢é˜Ÿè®¤ä¸ºéœ€è¦ä¸¥æ ¼æŒ‰ç…§â€œæ¨¡å‹åœ¨é—®é¢˜è¾“å…¥ä¸‹è‡ªç”±ç”Ÿæˆçš„ç­”æ¡ˆâ€è¿›è¡Œè¯„åˆ¤ï¼Œè¿™ç§æ–¹å¼è´´è¿‘ç”¨æˆ·çœŸå®ä½¿ç”¨åœºæ™¯ï¼Œå› æ­¤å‚è€ƒæ–¯å¦ç¦å¤§å­¦HELM[1]å·¥ä½œè¿›è¡Œè¯„æµ‹ï¼Œè¯¥è¯„æµ‹å¯¹äºæ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›è¦æ±‚æ›´ä¸ºä¸¥æ ¼ã€‚å®é™…è¯„æµ‹è¿‡ç¨‹ä¸­ï¼Œéƒ¨åˆ†å¯¹è¯æ¨¡å‹å›ç­”ä¸ç¬¦åˆæŒ‡ä»¤è¦æ±‚ï¼Œå¯èƒ½ä¼šå‡ºç°â€œ0â€åˆ†çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼šæ ¹æ®æŒ‡ä»¤è¦æ±‚ï¼Œæ­£ç¡®ç­”æ¡ˆä¸ºâ€œAâ€ï¼Œå¦‚æœæ¨¡å‹ç”Ÿæˆä¸ºâ€œBâ€æˆ–â€œç­”æ¡ˆæ˜¯ A â€ï¼Œéƒ½ä¼šè¢«åˆ¤ä¸ºâ€œ0â€åˆ†ã€‚åŒæ—¶ï¼Œä¸šå†…ä¹Ÿæœ‰å…¶ä»–è¯„æµ‹æ–¹å¼ï¼Œæ¯”å¦‚è®©å¯¹è¯æ¨¡å‹å…ˆæ‹¼æ¥â€œé—®é¢˜+ç­”æ¡ˆâ€ï¼Œæ¨¡å‹è®¡ç®—å„ä¸ªæ‹¼æ¥æ–‡æœ¬çš„æ¦‚ç‡åï¼ŒéªŒè¯æ¦‚ç‡æœ€é«˜çš„ç­”æ¡ˆä¸æ­£ç¡®ç­”æ¡ˆæ˜¯å¦ä¸€è‡´ï¼Œè¯„æµ‹è¿‡ç¨‹ä¸­å¯¹è¯æ¨¡å‹ä¸ä¼šç”Ÿæˆä»»ä½•å†…å®¹è€Œæ˜¯è®¡ç®—é€‰é¡¹æ¦‚ç‡ã€‚è¿™ç§è¯„æµ‹æ–¹å¼ä¸çœŸå®å¯¹è¯åœºæ™¯åå·®è¾ƒå¤§ï¼Œå› æ­¤åœ¨ç”Ÿæˆå¼å¯¹è¯æ¨¡å‹è¯„æµ‹ä¸­æ²¡æœ‰é‡‡çº³ã€‚
    <br>
    [1] https://crfm.stanford.edu/helm/latest
</p>
	
<br>


### é•¿æ–‡æœ¬ä»»åŠ¡è¡¨ç°

<br>

| Model                |   Method    | Avg. | EN-Avg. | ZH-Avg. | VCSUM(zh)<br>(Chinese) | LSHT(zh)<br>(Chinese) | HotpotQA<br>(English) | 2WikiMQA<br>(English) |
| :------------------- | :---------: | :--: | :-----: | :-----: | :--------------------: | :-------------------: | :-------------------: | :-------------------: |
| GPT-3.5-Turbo-16K   |      -      | 33.6 |  44.7   |  22.6   |          16.0          |         29.2          |         51.6          |         37.7          |
| **AquilaChat2-34B-16K** |  PI + SFT   | 32.8 |  44.1   |  21.5   |          16.5          |         26.5          |         47.4          |         40.8         |
| ChatGLM2-6B-32K     |  PI + SFT   | 30.8 |  39.6   |  22.0   |          16.2          |         27.7          |         45.1          |         34.0          |
| **AquilaChat2-7B-16K**  |  PI + SFT   | 29.5 |  31.7   |  27.2   |          14.4          |         40.0          |         36.1          |         27.3          |
| InternLM-7B-8K      |      -      | 22.4 |  30.6   |  14.3   |          13.0          |         15.5          |         33.3          |         27.9          |
| ChatGLM2-6B          |    None     | 22.1 |  26.6   |  17.6   |          14.6          |         20.5          |         33.0          |         20.2          |
| LongChat-7B-v1.5-32K |  PI + SFT   | 21.7 |  26.1   |  17.4   |          14.0          |         20.8          |         31.5          |         20.6          |
| Baichuan2-7B-Chat   |    None     | 21.3 |  25.9   |  16.8   |          13.6          |         20.0          |         32.8          |         18.9          |
| Internlm-20B-Chat   |    None     | 16.6 |  24.3   |   8.9   |          11.9          |          6.0          |         24.4          |         24.2          |
| Qwen-14B-Chat       | Dynamic NTK | 16.1 |  20.8   |  11.5   |          16.6          |          6.4          |         22.9          |         18.8          |
| XGen-7B-8K          |  Pre-train  | 16.0 |  21.3   |  10.8   |          1.5           |         20.0          |         14.2          |         28.3          |
| LLaMA2-7B-Chat-4K |    None     | 14.0 |  18.0   |  10.0   |          0.2           |         19.8          |         11.6          |         24.3          |
| Baichuan2-13B-Chat  |    None     | 10.5 |  14.8   |   6.3   |          7.0           |          5.5          |         16.0          |         13.6          |

<br>

### æ¨ç†ä»»åŠ¡è¡¨ç°

<br>

| Model                        | Avg. | bAbI#16<br>(Inductive) | CLUTRR<br>(Inductive) | bAbI#15<br>(Deductive) | EntailmentBank<br>(Deductive) | Î±NLI<br>(Abductive) | E-Care<br>(Casual) |
| :--------------------------- | :--: | :--------------------: | :-------------------: | :--------------------: | :---------------------------: | :-----------------: | :----------------: |
| Baichuan2-7B-Chat            | 47.8 |          40.0          |         26.7          |          43.3          |             73.3              |        53.3         |        50.0        |
| Qwen-7B-Chat                 | 49.5 |          20.0          |         10.0          |          66.7          |             86.7              |        56.7         |        56.7        |
| Qwen-14B-Chat                | 51.1 |          26.7          |         10.0          |          63.3          |             86.7              |        63.3         |        56.7        |
| Baichuan2-13B-Chat           | 53.3 |          33.3          |         10.0          |          66.7          |             80.0              |        66.7         |        63.3        |
| InternLM-20B-Chat            | 53.9 |          46.7          |         13.3          |          43.3          |             80.0              |        70.0         |        70.0        |
| ChatGPT                      | 55.6 |          46.7          |          6.7          |          86.7          |             83.3              |        63.3         |        46.7        |
| LLaMA-70B-Chat               | 57.2 |          63.3          |         20.0          |          53.3          |             80.0              |        66.7         |        60.0        |
| GPT-4                        | 81.1 |          93.3          |         36.7          |         100.0          |             90.0              |        83.3         |        83.3        |
| **AquilaChat2-34B**         | 58.3 |          43.3          |         16.7          |          63.6          |             80.0              |        80.0         |        66.7        |
| **AquilaChat2-34B+SFT**     | 65.6 |          73.3          |         16.7          |          76.7          |             80.0              |        76.7         |        70.0        |
| **AquilaChat2-34B+SFT+CoT** | 69.4 |          80.0          |         23.3          |          83.3          |             73.3              |        80.0         |        76.7        |

<br>

## å®‰è£…ç¯å¢ƒ

* python ç‰ˆæœ¬ >= 3.10 
* pytorch ç‰ˆæœ¬ >= 1.12, å»ºè®®2.0ç‰ˆæœ¬åŠä»¥ä¸Š
* transformers ç‰ˆæœ¬ >= 4.32
* CUDA ç‰ˆæœ¬ >= 11.4 (GPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹)

<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬ä¸ºæ‚¨å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹, æ¥æ¼”ç¤ºå¦‚ä½•å¿«é€Ÿä¸Šæ‰‹Aquila2.

åœ¨æ‚¨åŠ¨æ‰‹æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®è®¤æ‚¨å·²ç»è®¾ç½®å¥½äº†è¿è¡Œç¯å¢ƒï¼Œå¹¶æˆåŠŸå®‰è£…äº†å¿…è¦çš„ä»£ç åŒ…ã€‚é¦–å…ˆï¼Œè¯·ç¡®ä¿æ»¡è¶³è¿™äº›å…ˆå†³æ¡ä»¶ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢çš„æŒ‡ç¤ºå®‰è£…å¿…è¦çš„åº“å’Œä¾èµ–ã€‚


```
pip install -r requirements.txt
```

å¦‚æœæ‚¨çš„æ˜¾å¡å…¼å®¹ fp16 æˆ– bf16 ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜å»ºè®®æ‚¨å®‰è£… flash-attentionï¼Œä»¥å¢åŠ è¿è¡Œé€Ÿåº¦å’Œå‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚è¯·æ³¨æ„ï¼Œflash-attention ä¸æ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒæ‚¨ä¹Ÿèƒ½æ­£å¸¸æ‰§è¡Œè¯¥é¡¹ç›®ã€‚

flash-attentionå®‰è£…ï¼šå‚è€ƒ https://github.com/Dao-AILab/flash-attention/

### ä½¿ç”¨é•œåƒæ–‡ä»¶
å¯¹äºæ»¡è¶³è¿™ä¸ªè¦æ±‚çš„ç¯å¢ƒ[requirements](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-23-06.html)ï¼Œæ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ç›´æ¥[ä¸‹è½½dockeræ–‡ä»¶](https://model.baai.ac.cn/model-detail/220118)å¹¶å®‰è£…æ¥é…ç½®Aquila2æ‰€éœ€çš„ç¯å¢ƒã€‚

ç°åœ¨å¯ä»¥å¼€å§‹ä½¿ç”¨ <img src="assets/baai.png" width="18"/> Modelhub æˆ– ğŸ¤—Transformers æ¥è¿è¡Œæˆ‘ä»¬çš„æ¨¡å‹ã€‚


### <img src="assets/baai.png" width="20"/> ModelHub

è¦ä½¿ç”¨ Aquila2-Chat è¿›è¡Œæ¨ç†ï¼Œä½ åªéœ€è¦è¾“å…¥ä¸‹é¢æ¼”ç¤ºçš„å‡ è¡Œä»£ç ã€‚

```python
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'AquilaChat2-7B'
# model_name = 'AquilaChat2-34B'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_name)
# ä½¿ç”¨model_dirå‚æ•°è°ƒæ•´æ¨¡å‹åŠ è½½è·¯å¾„
# autoloader = AutoLoader("aquila2", model_dir='./checkpoints', model_name=model_name)
# å¦‚éœ€åŠ è½½LoRAæ¨¡å—ï¼Œéœ€è¦é¢å¤–æä¾›LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œlora_dir='./examples/checkpoints/lora/aquila2chat-hf')
# å¦‚éœ€åŠ è½½Q-LoRAæ¨¡å—ï¼Œéœ€è¦é¢å¤–æä¾›Q-LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œqlora_dir='./examples/checkpoints/qlora/aquila2chat-hf')

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()


# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer, model_name=model_name, top_p=0.9, seed=123, topk=15, temperature=1.0))
    # å¦‚æœæ˜¯åŸºç¡€æ¨¡å‹Aquila2-7Bæˆ–è€…Aquila2-34Bï¼Œéœ€è¦è®¾ç½® sft=False
    # print(model.predict(text, tokenizer=tokenizer, model_name=model_name, sft=False))
```

æˆ‘ä»¬è¿è¡Œçš„ç»“æœå¦‚ä¸‹:
```
åŒ—äº¬åå¤§æ™¯ç‚¹: 1. å¤©å®‰é—¨å¹¿åœº 2. æ•…å®« 3. é¢å’Œå›­ 4. å¤©å› 5. é¸Ÿå·¢ 6. åŒ—äº¬å¤§å­¦ 7. æ¸…åå¤§å­¦ 8. åŒ—äº¬åŠ¨ç‰©å›­ 9. åŒ—äº¬æ¤ç‰©å›­ 10. é•¿åŸã€‚

çšæ´æœˆå…‰æ´’ä¹æ´²ï¼Œå›¢åœ†ä½³èŠ‚å€æ€æ‚ ã€‚
```


### ğŸ¤— Transformers

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
device = torch.device("cuda:0")
model_info = "BAAI/AquilaChat2-7B"
tokenizer = AutoTokenizer.from_pretrained(model_info, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_info, trust_remote_code=True, torch_dtype=torch.bfloat16)
model.eval()
model.to(device)
text = "è¯·ç»™å‡º10ä¸ªè¦åˆ°åŒ—äº¬æ—…æ¸¸çš„ç†ç”±ã€‚"
from predict import predict
out = predict(model, text, tokenizer=tokenizer, max_gen_len=200, top_p=0.95,
              seed=1234, topk=100, temperature=0.9, sft=True, device=device,
              model_name="AquilaChat2-7B")
print(out)
```

### AquilaChat2-70B-Expr
é€šå¸¸éœ€è¦å¤šå¡è¿›è¡Œæ¨ç†å¦‚ä¸‹ï¼š
```python
from flagai.auto_model.auto_loader import AutoLoader

model_name = 'AquilaChat2-70B-Expr'

autoloader = AutoLoader("aquila2", model_name=model_name, all_devices=True)

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()

test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
    "Write a tongue twister that's extremely difficult to pronounce.",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer, model_name=model_name, top_p=0.9, seed=123, topk=15, temperature=1.0))
```
æ¨ç†ä¾‹å­ä¹Ÿå¯ä»¥å‚è€ƒ [AquilaChat2-70B-Expr](./examples/predict_chat_70b.py)ã€‚


## é‡åŒ–

### BitsAndBytesç”¨æ³•

ä½¿ç”¨é‡åŒ–ä¹‹å‰ï¼Œéœ€è¦å®‰è£…`BitsAndBytes`ï¼š

```
pip install bitsandbytes
```

æ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨é‡åŒ–æ¨¡å‹è¿›è¡Œæ¨ç†å•¦ï¼

```python
import torch 
from flagai.auto_model.auto_loader import AutoLoader
from transformers import BitsAndBytesConfig


model_name = 'AquilaChat2-7B'

autoloader = AutoLoader("aquila2", model_name=model_name, 
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    ))

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()
# 

test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
    "Write a tongue twister that's extremely difficult to pronounce.",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer, model_name=model_name, top_p=0.9, seed=123, topk=15, temperature=1.0))

```

AquilaChat2-34B 4Bit ç‰ˆæœ¬æ‹¥æœ‰99.3% bf16ç‰ˆæœ¬çš„æ€§èƒ½ã€‚

4Bit ç‰ˆæœ¬AquilaChat2-34Bæ‹¥æœ‰è¿œè¶…7Bçš„æ€§èƒ½ä¸7Bæ¨¡å‹æ¥è¿‘çš„æ˜¾å­˜å ç”¨ã€‚

<img src="./assets/table.png"   align=center />

### GPTQç”¨æ³•

é¦–å…ˆéœ€è¦æ‰‹åŠ¨ä¸‹è½½GPTQæ¨¡å‹(å½“å‰åªæœ‰34BChatæ¨¡å‹)ï¼Œç°åœ¨æ”¯æŒ[ModelScope](https://modelscope.cn/models/BAAI/AquilaChat2-34B-Int4-GPTQ/summary) å’Œ[WiseModel](https://www.wisemodel.cn/models/BAAI/AquilaChat2-34B-Int4-GPTQ/intro) | 

ç„¶åæ ¹æ®è‡ªå·±çš„ç¯å¢ƒï¼Œå‚è€ƒhttps://github.com/PanQiWei/AutoGPTQ/tree/main/auto_gptq/modeling é€‰æ‹©å®‰è£…æ–¹å¼ã€‚

æœ€åè¿è¡Œå¦‚ä¸‹ä»£ç å³å¯:

```python
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM

# pretrained_model_dir = "/share/project/ldwang/checkpoints/Aquila-33b-knowledge6-341000-sft-v0.9.16/iter_0004000_hf"
model_dir = "./checkpoints/Aquilachat34b-4bit" # æ¨¡å‹è·¯å¾„
device="cuda:0"

tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True,trust_remote_code=True)
model = AutoGPTQForCausalLM.from_quantized(model_dir, inject_fused_attention=False, low_cpu_mem_usage=True, device=device)


model.eval()
import time 
texts = ["è¯·ç»™å‡º10ä¸ªè¦åˆ°åŒ—äº¬æ—…æ¸¸çš„ç†ç”±ã€‚",
         "å†™ä¸€ä¸ªæ—é»›ç‰å€’æ‹”å‚æ¨æŸ³çš„æ•…äº‹",
         "write a poet about moon"]
from predict import predict
start_time = time.time()
for text in texts:
    out = predict(model, text, tokenizer=tokenizer, max_gen_len=200, top_p=0.95,
                seed=1234, topk=200, temperature=1.0, sft=True, device=device,
                model_name="AquilaChat2-34B")
print(out)
print(f"Elapsed time model loading: {time.time()-start_time} seconds")
```

### AWQç”¨æ³•
é¦–å…ˆè¿è¡Œ./examples/modelhub_download.pyæ¥ä¸‹è½½AquilaChat2-34B-AWQæ¨¡å‹ã€‚

ç„¶åä»https://github.com/casper-hansen/AutoAWQ å®‰è£…AutoAWQ==v0.1.5ã€‚

æœ€åè¿è¡Œå¦‚ä¸‹ä»£ç ï¼š
```python
import torch

from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

awq_model_path = './checkpoints/aquilachat2-34b-awq'
model = AutoAWQForCausalLM.from_quantized(awq_model_path,trust_remote_code=True,fuse_layers=True)
tokenizer = AutoTokenizer.from_pretrained(awq_model_path,trust_remote_code=True)
model.eval()

device = torch.device("cuda:0")
model.to(device)

text = "è¯·ç»™å‡º10ä¸ªè¦åˆ°åŒ—äº¬æ—…æ¸¸çš„ç†ç”±ã€‚"
from flagai.model.aquila2.utils import covert_prompt_to_input_ids_with_history
history = None
text = covert_prompt_to_input_ids_with_history(text, history, tokenizer, 2048, convo_template="aquila-legacy")
inputs = torch.tensor([text]).to(device)
outputs = model.generate(inputs)[0]
print(tokenizer.decode(outputs))
```

<br><br>

## å¾®è°ƒ

æˆ‘ä»¬ä¸ºç”¨æˆ·æä¾›äº†ä¸€ç³»åˆ—å¾®è°ƒè„šæœ¬ï¼Œç”¨äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨è„šæœ¬çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œç”¨æˆ·ä¼šæ‰¾åˆ°è¯¦ç»†çš„è¯´æ˜ï¼ŒæŒ‡æ˜å“ªäº›å‚æ•°éœ€è¦æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

åœ¨è¿›è¡Œå¾®è°ƒæ“ä½œä¹‹å‰ï¼Œæ‚¨å¿…é¡»å…ˆå‡†å¤‡å¥½æ‚¨çš„è®­ç»ƒæ•°æ®ã€‚æ‰€æœ‰æ ·æœ¬éœ€è¦é›†ä¸­åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œå¹¶å­˜å‚¨åœ¨ä¸€ä¸ª json æ–‡ä»¶é‡Œã€‚æ¯ä¸ªæ ·æœ¬åº”è¡¨ç°ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬ id å’Œ conversationï¼Œå…¶ä¸­ï¼Œconversation ä»¥åˆ—è¡¨çš„å½¢å¼å±•ç°ã€‚ä»¥ä¸‹æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ï¼š

```json
{
	"id": "alpaca_data.json_1",
	"conversations": [{
		"from": "human",
		"value": "What are the three primary colors?"
	}, {
		"from": "gpt",
		"value": "The three primary colors are red, blue, and yellow."
	}],
	"instruction": ""
}
```


ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›ä¸åŒçš„å¾®è°ƒè„šæœ¬å®ç°ä¸åŒåŠŸèƒ½ï¼š
- ä½¿ç”¨`finetune/7B/finetune.sh`å®ç°7Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_lora.sh`å®ç°7Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_qlora.sh`å®ç°7Bæ¨¡å‹Q-LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune.sh`å®ç°34Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_lora.sh`å®ç°34Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_qlora.sh`å®ç°34Bæ¨¡å‹Q-LoRAå¾®è°ƒ 

æ³¨æ„ï¼Œæ‚¨éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šè®­ç»ƒæ•°æ®çš„è·¯å¾„ï¼Œå¹¶ç›¸åº”åœ°é…ç½®hostfileã€‚

å¦‚æœåœ¨è„šæœ¬ä¸­æ²¡æœ‰æä¾›è‡ªå®šä¹‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œå®ƒå°†æ ¹æ®æŒ‡å®šçš„æ¨¡å‹åç§°è‡ªåŠ¨ä»ModelHubä¸‹è½½ç›¸åº”çš„æ¨¡å‹ï¼Œå¹¶ç»§ç»­è¿›è¡Œå¾®è°ƒæ“ä½œã€‚

è¦å®ç°å…¨å‚æ•°å¾®è°ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹è„šæœ¬ï¼š

```bash
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune.sh
```

LoRAçš„å¾®è°ƒæ–¹æ³•ï¼ˆå¦‚[è®ºæ–‡](https://arxiv.org/abs/2106.09685)ä¸­è¯¦ç»†æè¿°ï¼‰ä¸å…¨å‚å¾®è°ƒæœ‰æ‰€ä¸åŒã€‚LoRAä»…æ›´æ–°é€‚é…å™¨å±‚çš„å‚æ•°ï¼Œè€Œä¸ä¿®æ”¹åŸå§‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™ä¸€å®è·µå‡å°‘äº†å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚LoRAé€‚ç”¨äºå„ç§æ¨¡å‹å¤§å°å’Œä»»åŠ¡ï¼Œæœ‰åŠ©äºæ›´é«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ä»¥æ»¡è¶³ç‰¹å®šä»»åŠ¡æˆ–æ•°æ®é›†çš„éœ€æ±‚ã€‚

è¦å®ç°LoRAï¼Œè¯·è¿è¡Œä»¥ä¸‹è„šæœ¬ï¼š

```bash
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_lora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_lora.sh
```


å¦‚æœå†…å­˜èµ„æºä»ç„¶å—é™ï¼Œè¯·è€ƒè™‘ä½¿ç”¨Q-LoRAï¼ˆè¯·å‚è€ƒ[è®ºæ–‡](https://arxiv.org/abs/2305.14314)ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡ä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ä½¿ç”¨4ä½é‡åŒ–æ¨¡å‹å’Œåˆ†é¡µæ³¨æ„æŠ€æœ¯è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

è¦å®ç°Q-LoRAï¼Œè¯·è¿è¡Œä»¥ä¸‹è„šæœ¬ï¼š

```bash
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_qlora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_qlora.sh
```



### ä¼˜åŒ–æ•ˆæœ

ä»¥ä¸‹æ˜¯7Bå’Œ34Bæ¨¡å‹ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼ŒLoRA å’Œ QLoRA å¤„ç†ä¸åŒè¾“å…¥é•¿åº¦æ—¶çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æ•°æ®ã€‚è¯„æµ‹æ˜¯åœ¨ä¸€å°è£…å¤‡æœ‰ A100-SXM4-80G GPU çš„æœºå™¨ä¸Šè¿›è¡Œï¼Œä½¿ç”¨ CUDA 12.1 å’Œ Pytorch 2.1ã€‚å…¶ä¸­7Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º2048ï¼Œ 34Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º4096ã€‚æˆ‘ä»¬è¿›è¡Œçš„æ‰€æœ‰æµ‹è¯•å‡é‡‡ç”¨äº†æ‰¹æ¬¡å¤§å°ä¸º 4 å’Œæ¢¯åº¦ç´¯ç§¯ä¸º 1 çš„é…ç½®ï¼Œå¹¶ä¸”è®°å½•äº†ä»¥GBä¸ºå•ä½çš„æ˜¾å­˜å ç”¨å’Œä»¥s/iterä¸ºå•ä½çš„è®­ç»ƒé€Ÿåº¦ã€‚å…·ä½“çš„æ•°æ®å¦‚ä¸‹ï¼š

<table>
    <tr>
      <th>æ¨¡å‹å¤§å°</th><th>å¾®è°ƒæ–¹æ³•</th><th>æ˜¾å­˜å ç”¨</th><th>è®­ç»ƒé€Ÿåº¦</th>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td>SFT</td><td>43.9G</td><td>2.67s/iter</td>
    </tr>
    <tr>
        <td>LoRA</td><td>29.4G</td><td>2.04s/iter</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td>19.9G</td><td>2.14s/iter</td>
    </tr>
    <tr>
        <th rowspan="1">34B</th><td>Q-LoRA</td><td>37.7G</td><td>8.22s/iter</td>
    </tr>
</table>



<br><br>

## é¢„è®­ç»ƒ
ä»Aquila2å¼€å§‹ï¼Œæˆ‘ä»¬å‡çº§äº†åº•å±‚çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç°åœ¨ä»¥[FlagScale](https://github.com/FlagOpen/FlagScale)é¡¹ç›®è¿›è¡Œå¼€æºã€‚ç›®å‰ï¼Œå®ƒåŸºäºMegatron-LMé¡¹ç›®ï¼Œæ—¨åœ¨åœ¨ä¸ç‰ºç‰²æ•°å€¼ç¨³å®šæ€§å’Œæ¨¡å‹æœ‰æ•ˆæ€§çš„å‰æä¸‹ï¼Œé«˜æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºæ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚

åœ¨FlagScaleä¸­ï¼Œæˆ‘ä»¬ç‡å…ˆæä¾›äº†å®é™…è®­ç»ƒä¸­ä½¿ç”¨çš„Aquila2-7Bå’ŒAquila2-34Bçš„è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¹¶è¡Œç­–ç•¥ã€ä¼˜åŒ–é€‰æ‹©å’Œè¶…å‚æ•°è®¾ç½®ã€‚é€šè¿‡ä½¿ç”¨FlagScaleï¼Œæ¨¡å‹FLOPsåˆ©ç”¨ç‡åœ¨Aquila2-7Bå’ŒAquila2-34Bä¸Šå‡å¯è¾¾åˆ°å¾ˆé«˜æ°´å¹³ã€‚ç›®å‰ï¼ŒFlagScaleä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œæˆ‘ä»¬å°†ä¸ç¤¾åŒºå…±åŒåŠªåŠ›ï¼Œä»¥åœ¨ä¸åŒçš„ç¡¬ä»¶æ¶æ„ä¸Šæ”¯æŒå„ç§LLMsã€‚

## åº”ç”¨
ä¸€ç§åˆ©ç”¨langchainæ€æƒ³å®ç°çš„åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„é—®ç­”åº”ç”¨ï¼Œç›®æ ‡æœŸæœ›å»ºç«‹ä¸€å¥—å¯¹ä¸­è‹±åŒè¯­åœºæ™¯ä¸å¼€æºæ¨¡å‹æ”¯æŒå‹å¥½ã€å¯ç¦»çº¿è¿è¡Œçš„çŸ¥è¯†åº“é—®ç­”è§£å†³æ–¹æ¡ˆã€‚æœ¬é¡¹ç›®ä¾æ‰˜äºBAAIæ”¯æŒçš„å¼€æºLLMä¸Embeddingæ¨¡å‹ï¼Œå¯å®ç°å…¨éƒ¨ä½¿ç”¨å¼€æºæ¨¡å‹ç¦»çº¿ç§æœ‰éƒ¨ç½²ã€‚é¡¹ç›®å¯è§äº[Aquila_BGE_langchain](./examples/Aquila_BGE_langchain)ã€‚


## é•¿æ–‡æœ¬å¤„ç†
AquilaChat2-34B-16Kä»¥Aquila2-34Bä¸ºåŸºåº§ï¼Œç»è¿‡ä½ç½®ç¼–ç å†…æ’æ³•å¤„ç†ï¼Œå¹¶åœ¨20Wæ¡ä¼˜è´¨é•¿æ–‡æœ¬å¯¹è¯æ•°æ®é›†ä¸Šåšäº†SFTï¼Œå°†æ¨¡å‹çš„æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£é•¿åº¦æ‰©å±•è‡³16Kã€‚æˆ‘ä»¬åœ¨[LongBench](https://github.com/THUDM/LongBench)ä¸Šæµ‹è¯•äº†å››é¡¹ä¸­è‹±æ–‡é•¿æ–‡æœ¬é—®ç­”ã€æ€»ç»“ä»»åŠ¡ã€‚è¯„æµ‹æ•ˆæœæ˜¾ç¤ºï¼ŒAquilaChat2-34B-16Kå¤„äºå¼€æºé•¿æ–‡æœ¬æ¨¡å‹çš„é¢†å…ˆæ°´å¹³ï¼Œæ¥è¿‘ GPT-3.5-16kã€‚


## Tokenizer

æˆ‘ä»¬çš„ tokenizer æ˜¯ 50G å¤§å°æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ BBPE ç±»å‹ tokenizerã€‚æ•°æ®é›†ä¸»è¦ä»å»é‡åçš„Pileå’Œæ‚Ÿé“æ•°æ®é›†æŠ½æ ·å¾—åˆ°ã€‚

<br><br>

## FAQ

æ¬¢è¿åœ¨ [GitHub Issues](https://github.com/FlagAI-Open/Aquila2/issues) ä¸­æå‡ºä½ çš„é—®é¢˜æˆ–äº¤æµä½¿ç”¨ç»éªŒã€‚
<br><br>

## ä½¿ç”¨åè®®

Aquila2é¡¹ç›®åŸºäº [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0) åè®®ï¼›Aquila2ç³»åˆ—æ¨¡å‹åˆ™æ˜¯åŸºäº[æ™ºæºAquilaç³»åˆ—æ¨¡å‹è®¸å¯åè®®](./assets/aquila_license.pdf)ï¼›ç‰¹åˆ«è¯´æ˜ï¼ŒAquila2 70Bç³»åˆ—æ¨¡å‹åˆ™æ˜¯åŸºäº[æ™ºæºAquilaç³»åˆ—70Bæ¨¡å‹è®¸å¯åè®®](./assets/aquila2_70b_license.pdf)ã€‚

<br><br>

## è”ç³»æˆ‘ä»¬

* å®˜æ–¹é‚®ç®±ï¼šopen.platform@baai.ac.cnã€‚
* çŸ¥ä¹ï¼š[FlagAIé£æ™º](https://www.zhihu.com/people/95-22-20-18)
* æ‰«ç æ·»åŠ å°åŠ©æ‰‹åŠ å…¥**å¾®ä¿¡äº¤æµç¾¤**ï¼š

<img src="./assets/wechat-qrcode.jpg" width = "200" height = "200"  align=center />

