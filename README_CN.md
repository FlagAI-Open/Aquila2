<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
    <img src="./assets/logo.png" width="500"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/BAAI">Hugging Face</a>&nbsp&nbsp | &nbsp <a href="https://model.baai.ac.cn/models">ModelHub</a>&nbsp&nbsp | &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a> | &nbsp&nbsp <a href="assets/wechat-qrcode.png">å¾®ä¿¡</a>
</p>
<br><br>

---ä»‹ç»æˆ‘ä»¬è¿™æ¬¡å¼€æºäº†å“ªäº›æ¨¡å‹(7B/33B, baseå’Œchat)---

---åŠ ä¸€ä¸ªmodelhubé“¾æ¥çš„è¡¨æ ¼---

---ä»‹ç»ä¸€äº›Aquila2çš„ä¼˜åŠ¿---

---ç®€å•åˆ—ä¸€ä¸‹æ¥ä¸‹æ¥å¤§çº²---

---é‡åˆ°é—®é¢˜çš„è¯æ€ä¹ˆåŠï¼Œç„¶åå†æ”¾ä¸€æ³¢ç¤¾ç¾¤çš„é“¾æ¥---
<br><br>

## æ›´æ–°

* 2023å¹´10æœˆxæ—¥ï¼Œå‘å¸ƒAquila2 xxxç‰ˆæœ¬

## è¯„æµ‹è¡¨ç°(è¢é‡)

---ä»‹ç»---

---å¤šç»´å›¾---

---è¡¨æ ¼---

<br><br>

## å®‰è£…ç¯å¢ƒ

* Python ç‰ˆæœ¬ >= 3.8
* PyTorch ç‰ˆæœ¬ >= 1.8.0
* CUDA ç‰ˆæœ¬ >= 11.7ï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬ä¸ºæ‚¨å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹, æ¥æ¼”ç¤ºå¦‚ä½•å¿«é€Ÿä¸Šæ‰‹Aquila2.

åœ¨æ‚¨åŠ¨æ‰‹æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®è®¤æ‚¨å·²ç»è®¾ç½®å¥½äº†è¿è¡Œç¯å¢ƒï¼Œå¹¶æˆåŠŸå®‰è£…äº†å¿…è¦çš„ä»£ç åŒ…ã€‚é¦–å…ˆï¼Œè¯·ç¡®ä¿æ»¡è¶³è¿™äº›å…ˆå†³æ¡ä»¶ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢çš„æŒ‡ç¤ºå®‰è£…å¿…è¦çš„åº“å’Œä¾èµ–ã€‚

```
pip install -r requirements.txt
```

å¦‚æœæ‚¨çš„æ˜¾å¡å…¼å®¹ fp16 æˆ– bf16 ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜å»ºè®®æ‚¨å®‰è£… flash-attentionï¼Œä»¥å¢åŠ è¿è¡Œé€Ÿåº¦å’Œå‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚è¯·æ³¨æ„ï¼Œflash-attention ä¸æ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒæ‚¨ä¹Ÿèƒ½æ­£å¸¸æ‰§è¡Œè¯¥é¡¹ç›®ã€‚

flash-attentionå®‰è£…ï¼šå‚è€ƒ https://github.com/Dao-AILab/flash-attention/

### å¯¹è¯æ¨¡å‹æ¨ç†

æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨`AquilaChat2-7B`å¯¹è¯æ¨¡å‹æ¥è¿›è¡Œæ¨ç†ï¼š

```
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'AquilaChat2-7B'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_nameï¼‰
# ä½¿ç”¨model_dirå‚æ•°è°ƒæ•´æ¨¡å‹åŠ è½½è·¯å¾„
# autoloader = AutoLoader("aquila2", model_dir='./checkpoints', model_name=model_nameï¼‰
# å¦‚éœ€åŠ è½½LoRAæ¨¡å‹ï¼Œéœ€è¦é¢å¤–æä¾›LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œlora_dir='./examples/checkpoints/lora/aquila2chat-hf'ï¼‰
# å¦‚éœ€åŠ è½½Q-LoRAæ¨¡å‹ï¼Œéœ€è¦é¢å¤–æä¾›Q-LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œqlora_dir='./examples/checkpoints/qlora/aquila2chat-hf'ï¼‰

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()


# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer))
```

æˆ‘ä»¬è¿è¡Œçš„ç»“æœå¦‚ä¸‹:
```
model in: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­###Assistant:
åŒ—äº¬åå¤§æ™¯ç‚¹: 1. å¤©å®‰é—¨å¹¿åœº 2. æ•…å®« 3. é¢å’Œå›­ 4. å¤©å› 5. é¸Ÿå·¢ 6. åŒ—äº¬å¤§å­¦ 7. æ¸…åå¤§å­¦ 8. åŒ—äº¬åŠ¨ç‰©å›­ 9. åŒ—äº¬æ¤ç‰©å›­ 10. é•¿åŸã€‚
model in: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥###Assistant:
çšæ´æœˆå…‰æ´’ä¹æ´²ï¼Œå›¢åœ†ä½³èŠ‚å€æ€æ‚ ã€‚
```

### åŸºç¡€æ¨¡å‹æ¨ç†

åŸºç¡€æ¨¡å‹æ¨ç†ä¸å¯¹è¯æ¨¡å‹çš„ä¸åŒåœ¨äºæ¨¡å‹æ¨ç†çš„æ—¶å€™éœ€è¦è®¾ç½®`sft=False`
```
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'AquilaChat2-7B'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_name)

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()

# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer, sft=False))
```



## é‡åŒ–

### ç”¨æ³•

---é‡åŒ–ç”¨æ³•---

### æ•ˆæœè¯„æµ‹

---é‡åŒ–æ•ˆæœ(å¯é€‰)---

### æ¨ç†é€Ÿåº¦

---é‡åŒ–æ¨ç†é€Ÿåº¦(å¯é€‰)---

### æ˜¾å­˜ä½¿ç”¨

---é‡åŒ–æ˜¾å­˜ä½¿ç”¨(å¯é€‰)---
<br><br>

## å¾®è°ƒ

æˆ‘ä»¬ä¸ºç”¨æˆ·æä¾›äº†ä¸€ç³»åˆ—å¾®è°ƒè„šæœ¬ï¼Œç”¨äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨è„šæœ¬çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œç”¨æˆ·ä¼šæ‰¾åˆ°è¯¦ç»†çš„è¯´æ˜ï¼ŒæŒ‡æ˜å“ªäº›å‚æ•°éœ€è¦æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

åœ¨è¿›è¡Œå¾®è°ƒæ“ä½œä¹‹å‰ï¼Œæ‚¨å¿…é¡»å…ˆå‡†å¤‡å¥½æ‚¨çš„è®­ç»ƒæ•°æ®ã€‚æ‰€æœ‰æ ·æœ¬éœ€è¦é›†ä¸­åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œå¹¶å­˜å‚¨åœ¨ä¸€ä¸ª json æ–‡ä»¶é‡Œã€‚æ¯ä¸ªæ ·æœ¬åº”è¡¨ç°ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬ id å’Œ conversationï¼Œå…¶ä¸­ï¼Œconversation ä»¥åˆ—è¡¨çš„å½¢å¼å±•ç°ã€‚ä»¥ä¸‹æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ï¼š

```
{"id": "alpaca_data.json_1", "conversations": [{"from": "human", "value": "What are the three primary colors?"}, {"from": "gpt", "value": "The three primary colors are red, blue, and yellow."}], "instruction": ""}
```

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›ä¸åŒçš„å¾®è°ƒè„šæœ¬å®ç°ä¸åŒåŠŸèƒ½ï¼š
- ä½¿ç”¨`finetune/7B/finetune.sh`å®ç°7Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_lora.sh`å®ç°7Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/7B/finetune_qlora.sh`å®ç°7Bæ¨¡å‹Q-LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune.sh`å®ç°34Bæ¨¡å‹å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_lora.sh`å®ç°34Bæ¨¡å‹LoRAå¾®è°ƒ 
- ä½¿ç”¨`finetune/34B/finetune_qlora.sh`å®ç°34Bæ¨¡å‹Q-LoRAå¾®è°ƒ 

æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šè®­ç»ƒæ•°æ®çš„è·¯å¾„, å¹¶é…ç½®hostfileæ–‡ä»¶ã€‚è‹¥æœªåœ¨è„šæœ¬é‡Œæä¾›è‡ªå®šä¹‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œè„šæœ¬å°†ä¼šåŸºäºæŒ‡å®šçš„æ¨¡å‹åç§°è‡ªåŠ¨ä» ModelHub ä¸‹è½½ç›¸åº”çš„æ¨¡å‹ï¼Œå¹¶æ‰§è¡Œå¾®è°ƒæ“ä½œã€‚


å®ç°å…¨å‚æ•°å¾®è°ƒåªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬

```
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune.sh
```

LoRA (å‚è§[è®ºæ–‡](https://arxiv.org/abs/2106.09685)) çš„å¾®è°ƒæ–¹æ³•ä¸å…¨å‚æ•°å¾®è°ƒæœ‰æ‰€ä¸åŒã€‚LoRA ä»…æ›´æ–° adapter å±‚çš„å‚æ•°ï¼Œè€Œä¸æ›´æ–°åŸå§‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™æ ·åšå¯ä»¥å‡å°æ˜¾å­˜å’Œè®¡ç®—å¼€é”€ï¼ŒLoRA é€‚ç”¨äºå„ç§ä¸åŒå¤§å°çš„æ¨¡å‹å’Œå„ç§ä¸åŒçš„ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´é«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ä»¥é€‚åº”ç‰¹å®šçš„ä»»åŠ¡æˆ–æ•°æ®é›†ã€‚

å®ç°LORAåªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬
```
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_lora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_lora.sh
```

å¦‚æœæ˜¾å­˜èµ„æºä»ç„¶å—é™ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ Q-LoRA (å‚è§[è®ºæ–‡](https://arxiv.org/abs/2305.14314))ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹å’Œ paged attention æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜ä½¿ç”¨çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

å®ç°Q-LoRAåªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬

```
# å¾®è°ƒ7Bæ¨¡å‹
bash finetune/7B/finetune_qlora.sh
# å¾®è°ƒ34Bæ¨¡å‹
bash finetune/34B/finetune_qlora.sh
```




### ä¼˜åŒ–æ•ˆæœ

ä»¥ä¸‹æ˜¯7Bå’Œ34Bæ¨¡å‹ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼ŒLoRA å’Œ QLoRA å¤„ç†ä¸åŒè¾“å…¥é•¿åº¦æ—¶çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æ•°æ®ã€‚è¯„æµ‹æ˜¯åœ¨ä¸€å°è£…å¤‡æœ‰ A100-SXM4-80G GPU çš„æœºå™¨ä¸Šè¿›è¡Œï¼Œä½¿ç”¨ CUDA 12.1 å’Œ Pytorch 2.1ã€‚å…¶ä¸­7Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º2048ï¼Œ 34Bæ¨¡å‹çš„è¾“å…¥é•¿åº¦ä¸º4096ã€‚æˆ‘ä»¬è¿›è¡Œçš„æ‰€æœ‰æµ‹è¯•å‡é‡‡ç”¨äº†æ‰¹æ¬¡å¤§å°ä¸º 4 å’Œæ¢¯åº¦ç´¯ç§¯ä¸º 1 çš„é…ç½®ï¼Œå¹¶ä¸”è®°å½•äº†ä»¥GBä¸ºå•ä½çš„æ˜¾å­˜å ç”¨å’Œä»¥s/iterä¸ºå•ä½çš„è®­ç»ƒé€Ÿåº¦ã€‚å…·ä½“çš„æ•°æ®å¦‚ä¸‹ï¼š

<table>
    <tr>
      <th>Model Size</th><th>Method</th><th>Memory</th><th>speed</th>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td>SFT</td><td>43.9G</td><td>2.67s/iter</td>
    </tr>
    <tr>
        <td>LoRA</td><td>29.4G</td><td>2.04s/iter</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td>19.9G</td><td>2.14s/iter</td>
    </tr>
    <tr>
        <th rowspan="2">34B</th><td>LoRA</td><td>LoRA</td><td>LoRA</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td>8.22s/it</td><td>37.7G</td>
    </tr>
</table>



<br><br>

## é¢„è®­ç»ƒ

---é¢„è®­ç»ƒä½¿ç”¨(ç‰é¾™)---
<br><br>

## é•¿æ–‡æœ¬ç†è§£

---ä»‹ç»---

---è¯„æµ‹ç»“æœ---

## Tokenization

---ä¸­æ–‡å¯ä»¥ç®€å•è¯´è¯´tokenizationæ˜¯ä»€ä¹ˆï¼ˆå› ä¸ºè¿™è¯æ²¡æœ‰å¥½çš„ä¸­æ–‡å¯¹åº”ç¿»è¯‘ï¼‰---

---ç»™ä¸€ä¸ªtokenizeræ–‡æ¡£çš„link(å¯é€‰)---
<br><br>

## å¤ç°

---å¤ç°è¯„æµ‹çš„è„šæœ¬(å¯é€‰)---
<br><br>

## FAQ

æ¬¢è¿åœ¨ [GitHub Issues](https://github.com/FlagAI-Open/FlagAI/issues) ä¸­æå‡ºä½ çš„é—®é¢˜ï¼Œæˆ–åœ¨ [Discussions ](https://github.com/FlagAI-Open/FlagAI/discussions) æ¿å—äº¤æµä½¿ç”¨ç»éªŒã€‚

---ä¹‹åå¯ä»¥å¼„ä¸€ä¸ªå¸¸è§é—®é¢˜çš„æ–‡æ¡£linkæ”¾åˆ°è¿™é‡Œ---
<br><br>

## ä½¿ç”¨åè®®

FlagAIé£æ™ºå¤§éƒ¨åˆ†é¡¹ç›®åŸºäº [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)
---å¯èƒ½è¿˜éœ€è¦è¡¥å……---
<br><br>

## è”ç³»æˆ‘ä»¬

* å®˜æ–¹é‚®ç®±ï¼šopen.platform@baai.ac.cnã€‚
* çŸ¥ä¹ï¼š[FlagAIé£æ™º](https://www.zhihu.com/people/95-22-20-18)
* æ‰«ç æ·»åŠ å°åŠ©æ‰‹åŠ å…¥**å¾®ä¿¡äº¤æµç¾¤**ï¼š

<img src="./assets/wechat-qrcode.jpg" width = "200" height = "200"  align=center />

