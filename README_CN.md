<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br><br>

<p align="center">
    <img src="./assets/logo.png" width="500"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/BAAI/AquilaChat-7B">Hugging Face</a>&nbsp&nbsp | &nbsp <a href="https://model.baai.ac.cn/models">ModelHub</a>&nbsp&nbsp | &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a> | &nbsp&nbsp <a href="assets/wechat-qrcode.png">å¾®ä¿¡</a>
</p>
<br><br>

---ä»‹ç»æˆ‘ä»¬è¿™æ¬¡å¼€æºäº†å“ªäº›æ¨¡å‹(7B/33B, baseå’Œchat)---

---åŠ ä¸€ä¸ªmodelhubé“¾æ¥çš„è¡¨æ ¼---

---ä»‹ç»ä¸€äº›Aquila2çš„ä¼˜åŠ¿---

---ç®€å•åˆ—ä¸€ä¸‹æ¥ä¸‹æ¥å¤§çº²---

---é‡åˆ°é—®é¢˜çš„è¯æ€ä¹ˆåŠï¼Œç„¶åå†æ”¾ä¸€æ³¢ç¤¾ç¾¤çš„é“¾æ¥---
<br><br>

## æ›´æ–°

* 2023å¹´10æœˆxæ—¥ï¼Œå‘å¸ƒAquila2 xxxç‰ˆæœ¬

## è¯„æµ‹è¡¨ç°(è¢é‡)

---ä»‹ç»---

---å¤šç»´å›¾---

---è¡¨æ ¼---

<br><br>

## å®‰è£…ç¯å¢ƒ

* Python ç‰ˆæœ¬ >= 3.8
* PyTorch ç‰ˆæœ¬ >= 1.8.0
* CUDA ç‰ˆæœ¬ >= 11.7ï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬ä¸ºæ‚¨å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹, æ¥æ¼”ç¤ºå¦‚ä½•å¿«é€Ÿä¸Šæ‰‹Aquila2.

åœ¨æ‚¨åŠ¨æ‰‹æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®è®¤æ‚¨å·²ç»è®¾ç½®å¥½äº†è¿è¡Œç¯å¢ƒï¼Œå¹¶æˆåŠŸå®‰è£…äº†å¿…è¦çš„ä»£ç åŒ…ã€‚é¦–å…ˆï¼Œè¯·ç¡®ä¿æ»¡è¶³è¿™äº›å…ˆå†³æ¡ä»¶ï¼Œç„¶åæŒ‰ç…§ä¸‹é¢çš„æŒ‡ç¤ºå®‰è£…å¿…è¦çš„åº“å’Œä¾èµ–ã€‚

```
pip install -r requirements.txt
```

å¦‚æœæ‚¨çš„æ˜¾å¡å…¼å®¹ fp16 æˆ– bf16 ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜å»ºè®®æ‚¨å®‰è£… flash-attentionï¼Œä»¥å¢åŠ è¿è¡Œé€Ÿåº¦å’Œå‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚è¯·æ³¨æ„ï¼Œflash-attention ä¸æ˜¯å¿…é¡»çš„ï¼Œæ²¡æœ‰å®ƒæ‚¨ä¹Ÿèƒ½æ­£å¸¸æ‰§è¡Œè¯¥é¡¹ç›®ã€‚

flash-attentionå®‰è£…ï¼šå‚è€ƒ https://github.com/Dao-AILab/flash-attention/

### å¯¹è¯æ¨¡å‹æ¨ç†

æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨`AquilaChat2-7B`å¯¹è¯æ¨¡å‹æ¥è¿›è¡Œæ¨ç†ï¼š

```
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'Aquila2Chat-hf'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_nameï¼‰
# ä½¿ç”¨model_dirå‚æ•°è°ƒæ•´æ¨¡å‹åŠ è½½è·¯å¾„
# autoloader = AutoLoader("aquila2", model_dir='./checkpoints', model_name=model_nameï¼‰
# å¦‚éœ€åŠ è½½LoRAæ¨¡å‹ï¼Œéœ€è¦é¢å¤–æä¾›LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œlora_dir='./examples/checkpoints/lora/aquila2chat-hf'ï¼‰
# å¦‚éœ€åŠ è½½Q-LoRAæ¨¡å‹ï¼Œéœ€è¦é¢å¤–æä¾›Q-LoRAæ¨¡å—çš„åœ°å€
# autoloader = AutoLoader("aquila2", model_name=model_nameï¼Œqlora_dir='./examples/checkpoints/qlora/aquila2chat-hf'ï¼‰

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()


# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer))
```

æˆ‘ä»¬è¿è¡Œçš„ç»“æœå¦‚ä¸‹:
```
model in: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­###Assistant:
ä»¥ä¸‹æ˜¯åŒ—äº¬çš„åå¤§æ™¯ç‚¹åŠå…¶ç¿»è¯‘:

1. æ•…å®«åšç‰©é™¢ - Palace Museum (tÅ-gÅ« shisÅ hokusei-en)

2. å¤©å›å…¬å›­ - Tiantan Park (tÄn-tÄng kÅen)

3. é¢å’Œå›­ - Yingge Garden (yÃ­hÃ© yuÃ¡n)

4. é•¿åŸ - Great Wall (dÃ  chÃ©ng)

5. é¸Ÿå·¢ - Bird's Nest (hÃ³ngtÇ’ng)

6. åŒ—äº¬å¤§å­¦ - Peking University (bei-jing dÃ xuÃ©)

7. ç‹åºœäº•å°åƒè¡— - Wangfujing Snack Street (wÃ¡ngfÃºjÇng kÇo dÃ¬)

8. æ­ç‹åºœ - Gong Palace (gÅng wÇ” fÇ”)

9. æ¸…åå¤§å­¦ - T
model in: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥###Assistant:
æœˆåˆ°ä¸­ç§‹åˆ†å¤–æ˜ï¼Œ
å›¢åœ†ç¾æ»¡åº¦ä½³èŠ‚ã€‚
äººé—´å…±åº¦äº²æƒ…æœˆï¼Œ
å®¶å’Œä¸‡äº‹å‡å¦‚æ„ã€‚
```

### åŸºç¡€æ¨¡å‹æ¨ç†

åŸºç¡€æ¨¡å‹æ¨ç†ä¸å¯¹è¯æ¨¡å‹çš„ä¸åŒåœ¨äºæ¨¡å‹æ¨ç†çš„æ—¶å€™éœ€è¦è®¾ç½®`sft=False`
```
from flagai.auto_model.auto_loader import AutoLoader


# æ¨¡å‹åç§°
model_name = 'Aquila2Chat-hf'

# åŠ è½½æ¨¡å‹ä»¥åŠtokenizer
autoloader = AutoLoader("aquila2", model_name=model_name)

model = autoloader.get_model()
tokenizer = autoloader.get_tokenizer()

# å¯¹è¯æµ‹è¯•æ ·ä¾‹
test_data = [
    "åŒ—äº¬çš„åå¤§æ™¯ç‚¹æ˜¯ä»€ä¹ˆ?è¯·å°†å›ç­”ç¿»è¯‘æˆè‹±æ–‡å’Œæ—¥è¯­",
    "å†™ä¸€é¦–ä¸­ç§‹ä¸»é¢˜çš„äº”è¨€ç»å¥",
]

for text in test_data:
    print(model.predict(text, tokenizer=tokenizer, sft=False))
```



## é‡åŒ–

### ç”¨æ³•

---é‡åŒ–ç”¨æ³•---

### æ•ˆæœè¯„æµ‹

---é‡åŒ–æ•ˆæœ(å¯é€‰)---

### æ¨ç†é€Ÿåº¦

---é‡åŒ–æ¨ç†é€Ÿåº¦(å¯é€‰)---

### æ˜¾å­˜ä½¿ç”¨

---é‡åŒ–æ˜¾å­˜ä½¿ç”¨(å¯é€‰)---
<br><br>

## å¾®è°ƒ

æˆ‘ä»¬ä¸ºç”¨æˆ·æä¾›äº†ä¸€ç³»åˆ—å¾®è°ƒè„šæœ¬ï¼Œç”¨äºåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä»¥é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨è„šæœ¬çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œç”¨æˆ·ä¼šæ‰¾åˆ°è¯¦ç»†çš„è¯´æ˜ï¼ŒæŒ‡æ˜å“ªäº›å‚æ•°éœ€è¦æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

åœ¨è¿›è¡Œå¾®è°ƒæ“ä½œä¹‹å‰ï¼Œæ‚¨å¿…é¡»å…ˆå‡†å¤‡å¥½æ‚¨çš„è®­ç»ƒæ•°æ®ã€‚æ‰€æœ‰æ ·æœ¬éœ€è¦é›†ä¸­åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œå¹¶å­˜å‚¨åœ¨ä¸€ä¸ª json æ–‡ä»¶é‡Œã€‚æ¯ä¸ªæ ·æœ¬åº”è¡¨ç°ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬ id å’Œ conversationï¼Œå…¶ä¸­ï¼Œconversation ä»¥åˆ—è¡¨çš„å½¢å¼å±•ç°ã€‚ä»¥ä¸‹æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ï¼š

```
{"id": "alpaca_data.json_1", "conversations": [{"from": "human", "value": "What are the three primary colors?"}, {"from": "gpt", "value": "The three primary colors are red, blue, and yellow."}], "instruction": ""}
```

å¤‡å¥½æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„shellè„šæœ¬å®ç°å¾®è°ƒã€‚æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šä½ çš„æ•°æ®çš„è·¯å¾„ã€‚

è‹¥æœªæä¾›è‡ªå®šä¹‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œè„šæœ¬å°†ä¼šåŸºäºæŒ‡å®šçš„æ¨¡å‹åç§°è‡ªåŠ¨ä» ModelHub ä¸‹è½½ç›¸åº”çš„æ¨¡å‹ï¼Œå¹¶æ‰§è¡Œå¾®è°ƒæ“ä½œã€‚

å…ˆè¿›å…¥`./examples`ç›®å½•
```
cd examples
```

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨ä¸åŒçš„å¾®è°ƒè„šæœ¬å®ç°ä¸åŒåŠŸèƒ½ï¼š
- ä½¿ç”¨`./finetune.sh`å®ç°å…¨å‚æ•°å¾®è°ƒ 
- ä½¿ç”¨`./finetune_lora.sh`å®ç°LoRAå¾®è°ƒ 
- ä½¿ç”¨`./finetune_qlora.sh`å®ç°Q-LoRAå¾®è°ƒ 



å®ç°å…¨å‚æ•°å¾®è°ƒåªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬

```
bash finetune.sh

```

LoRA (å‚è§[è®ºæ–‡](https://arxiv.org/abs/2106.09685)) çš„å¾®è°ƒæ–¹æ³•ä¸å…¨å‚æ•°å¾®è°ƒæœ‰æ‰€ä¸åŒã€‚LoRA ä»…æ›´æ–° adapter å±‚çš„å‚æ•°ï¼Œè€Œä¸æ›´æ–°åŸå§‹è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™æ ·åšå¯ä»¥å‡å°æ˜¾å­˜å’Œè®¡ç®—å¼€é”€ï¼ŒLoRA é€‚ç”¨äºå„ç§ä¸åŒå¤§å°çš„æ¨¡å‹å’Œå„ç§ä¸åŒçš„ä»»åŠ¡ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´é«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ä»¥é€‚åº”ç‰¹å®šçš„ä»»åŠ¡æˆ–æ•°æ®é›†ã€‚

å®ç°LORAåªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬
```
bash finetune_lora.sh
```

å¦‚æœæ˜¾å­˜èµ„æºä»ç„¶å—é™ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ Q-LoRA (å‚è§[è®ºæ–‡](https://arxiv.org/abs/2305.14314))ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹å’Œ paged attention æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜ä½¿ç”¨çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

å®ç°Q-LoRAåªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬

```
bash finetune_qlora.sh
```





### ä¼˜åŒ–æ•ˆæœ

7B å…¨å‚, 2048: 2.67s/it, 43.9G
lora: 2.04s/it, 29.4G
qlora: 2.14s/it, 19.9G

34B, qlora, 37.7G, 8.22s/it



<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Method</th><th colspan="4" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">256</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th>
    </tr>
    <tr>
        <th rowspan="2">7B</th><td>LoRA</td><td align="center">33.5G / 1.6s/it</td><td align="center">34.0G / 1.7s/it</td><td align="center">35.0G / 3.0s/it</td><td align="center">35.0G / 5.7s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">11.5G / 3.0s/it</td><td align="center">12.2G / 3.6s/it</td><td align="center">12.7G / 4.8s/it</td><td align="center">13.9G / 7.3s/it</td>
    </tr>
    <tr>
        <th rowspan="2">14B</th><td>LoRA</td><td align="center">51.0G / 2.1s/it</td><td align="center">51.0G / 2.7s/it</td><td align="center">51.5G / 5.0s/it</td><td align="center">53.9G / 9.2s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">18.3G / 5.4s/it</td><td align="center">18.4G / 6.4s/it</td><td align="center">18.5G / 8.5s/it</td><td align="center">19.9G / 12.4s/it</td>
    </tr>
</table>

<br><br>

## é¢„è®­ç»ƒ

---é¢„è®­ç»ƒä½¿ç”¨(ç‰é¾™)---
<br><br>

## é•¿æ–‡æœ¬ç†è§£

---ä»‹ç»---

---è¯„æµ‹ç»“æœ---

## Tokenization

---ä¸­æ–‡å¯ä»¥ç®€å•è¯´è¯´tokenizationæ˜¯ä»€ä¹ˆï¼ˆå› ä¸ºè¿™è¯æ²¡æœ‰å¥½çš„ä¸­æ–‡å¯¹åº”ç¿»è¯‘ï¼‰---

---ç»™ä¸€ä¸ªtokenizeræ–‡æ¡£çš„link(å¯é€‰)---
<br><br>

## å¤ç°

---å¤ç°è¯„æµ‹çš„è„šæœ¬(å¯é€‰)---
<br><br>

## FAQ

æ¬¢è¿åœ¨ [GitHub Issues](https://github.com/FlagAI-Open/FlagAI/issues) ä¸­æå‡ºä½ çš„é—®é¢˜ï¼Œæˆ–åœ¨ [Discussions ](https://github.com/FlagAI-Open/FlagAI/discussions) æ¿å—äº¤æµä½¿ç”¨ç»éªŒã€‚

---ä¹‹åå¯ä»¥å¼„ä¸€ä¸ªå¸¸è§é—®é¢˜çš„æ–‡æ¡£linkæ”¾åˆ°è¿™é‡Œ---
<br><br>

## ä½¿ç”¨åè®®

FlagAIé£æ™ºå¤§éƒ¨åˆ†é¡¹ç›®åŸºäº [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0)
---å¯èƒ½è¿˜éœ€è¦è¡¥å……---
<br><br>

## è”ç³»æˆ‘ä»¬

* å®˜æ–¹é‚®ç®±ï¼šopen.platform@baai.ac.cnã€‚
* çŸ¥ä¹ï¼š[FlagAIé£æ™º](https://www.zhihu.com/people/95-22-20-18)
* æ‰«ç æ·»åŠ å°åŠ©æ‰‹åŠ å…¥**å¾®ä¿¡äº¤æµç¾¤**ï¼š

<img src="./assets/wechat-qrcode.jpg" width = "200" height = "200"  align=center />

